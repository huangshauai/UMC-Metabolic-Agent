# -*- coding: utf-8 -*-
"""
UMC-Metabolic-Agent é€šç”¨æ‰¹é‡å¤„ç†è„šæœ¬
æ ¸å¿ƒé€»è¾‘ï¼šè½»é‡åŒ–æ‰¹é‡ä»»åŠ¡å¤„ç†ï¼Œæ”¯æŒè‡ªå®šä¹‰ä»»åŠ¡åˆ—è¡¨ã€å¤šä»»åŠ¡ç±»å‹ã€çµæ´»å¹¶è¡Œï¼Œé€‚é…é€šç”¨æ‰¹é‡åœºæ™¯
è®¾è®¡åŸåˆ™ï¼šé…ç½®çµæ´»ã€æ˜“ç”¨æ€§å¼ºã€è¾“å‡ºç®€æ´ã€æ‰©å±•æ–¹ä¾¿ï¼Œå…¼é¡¾æ–°æ‰‹å’Œè¿›é˜¶ç”¨æˆ·
"""
import os
import sys
import json
import time
import logging
import warnings
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------------------------ åŸºç¡€é…ç½®ä¸æ—¥å¿— ------------------------------
# é¢œè‰²è¾“å‡ºå·¥å…·ï¼ˆå¢å¼ºå¯è¯»æ€§ï¼‰
class Color:
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    PURPLE = '\033[35m'
    CYAN = '\033[36m'
    RESET = '\033[0m'

# æ—¥å¿—é…ç½®ï¼ˆè½»é‡åŒ–ï¼Œå¸¦ä»»åŠ¡æ ‡è¯†ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format=f"{Color.BLUE}[%(asctime)s]{Color.RESET} [{Color.PURPLE}%(task_id)s{Color.RESET}] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("UMC-BatchProcess")
warnings.filterwarnings("ignore")

# é»˜è®¤æ‰¹é‡é…ç½®ï¼ˆå¯é€šè¿‡å‘½ä»¤è¡Œ/é…ç½®æ–‡ä»¶è¦†ç›–ï¼‰
DEFAULT_CONFIG = {
    "output_dir": "./umc_batch_process_output",  # æ‰¹é‡è¾“å‡ºæ ¹ç›®å½•
    "parallel_workers": 2,                       # å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
    "task_type": "tune",                         # é»˜è®¤ä»»åŠ¡ç±»å‹ï¼šrun/tune/analyze/all
    "generate_data": True,                       # æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®
    "data_rows": 1000,                           # æ¯ä¸ªä»»åŠ¡çš„æµ‹è¯•æ•°æ®è¡Œæ•°
    "save_individual_report": True,              # æ˜¯å¦ä¿å­˜å•ä¸ªä»»åŠ¡æŠ¥å‘Š
    "save_batch_summary": True,                  # æ˜¯å¦ä¿å­˜æ‰¹é‡æ±‡æ€»
    "overwrite": False,                          # æ˜¯å¦è¦†ç›–å·²æœ‰ç»“æœ
    
    # æ‰¹é‡ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒå¤šå‚æ•°/å¤šé¢†åŸŸç»„åˆï¼‰
    "tasks": [
        {"task_id": "task_quantum_001", "domain": "quantum", "iter": 30, "lr": 0.01},
        {"task_id": "task_quantum_002", "domain": "quantum", "iter": 50, "lr": 0.01},
        {"task_id": "task_biology_001", "domain": "biology", "iter": 30, "lr": 0.008},
        {"task_id": "task_chemistry_001", "domain": "chemistry", "iter": 40, "lr": 0.015}
    ]
}

# ------------------------------ ä¾èµ–æ£€æŸ¥ä¸æ¨¡å—å¯¼å…¥ ------------------------------
def check_dependencies():
    """æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…æ ¸å¿ƒä¾èµ–"""
    required = ["pandas", "numpy", "matplotlib"]
    missing = []
    for pkg in required:
        try:
            __import__(pkg)
        except ImportError:
            missing.append(pkg)
    
    if missing:
        logger.warning(f"{Color.YELLOW}ç¼ºå¤±ä¾èµ–ï¼š{', '.join(missing)}ï¼Œè‡ªåŠ¨å®‰è£…...{Color.RESET}")
        try:
            import subprocess
            subprocess.check_call(
                [sys.executable, "-m", "pip", "install", "--quiet"] + missing,
                stdout=subprocess.DEVNULL
            )
            logger.info(f"{Color.GREEN}ä¾èµ–å®‰è£…å®Œæˆ{Color.RESET}")
        except Exception as e:
            logger.error(f"{Color.RED}ä¾èµ–å®‰è£…å¤±è´¥ï¼š{e}{Color.RESET}")
            sys.exit(1)

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.getcwd())

# æ ¸å¿ƒæ¨¡å—å¯¼å…¥
try:
    check_dependencies()
    from universal_cmd import UniversalCmd
    from tuner_dashboard import TunerDashboard
    from result_analysis import ResultAnalyzer
    MODULE_LOADED = True
    logger.info(f"{Color.GREEN}âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ{Color.RESET}")
except Exception as e:
    logger.error(f"{Color.RED}âŒ æ¨¡å—å¯¼å…¥å¤±è´¥ï¼š{e}{Color.RESET}")
    logger.error(f"{Color.YELLOW}è¯·ç¡®ä¿æ ¸å¿ƒæ–‡ä»¶ï¼ˆuniversal_cmd.py/tuner_dashboard.pyï¼‰åœ¨å½“å‰ç›®å½•{Color.RESET}")
    sys.exit(1)

# ------------------------------ é€šç”¨å·¥å…·å‡½æ•° ------------------------------
def load_config(config_path: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œæ— åˆ™ä½¿ç”¨é»˜è®¤é…ç½®"""
    config = DEFAULT_CONFIG.copy()
    if config_path and os.path.exists(config_path):
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                custom_config = json.load(f)
            config.update(custom_config)
            logger.info(f"{Color.BLUE}ğŸ“„ åŠ è½½è‡ªå®šä¹‰é…ç½®ï¼š{config_path}{Color.RESET}")
        except Exception as e:
            logger.error(f"{Color.RED}åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®{Color.RESET}")
    return config

def ensure_dir(dir_path: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        logger.info(f"{Color.BLUE}ğŸ“ åˆ›å»ºç›®å½•ï¼š{dir_path}{Color.RESET}")

def generate_task_data(task_id: str, config: dict) -> str:
    """ä¸ºå•ä¸ªä»»åŠ¡ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    data_dir = f"{config['output_dir']}/test_data"
    ensure_dir(data_dir)
    data_path = f"{data_dir}/{task_id}_data.csv"
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸è¦†ç›–ï¼Œç›´æ¥è¿”å›
    if os.path.exists(data_path) and not config["overwrite"]:
        logger.info(f"{Color.BLUE}ğŸ“„ ä»»åŠ¡{task_id}ä½¿ç”¨å·²æœ‰æ•°æ®ï¼š{data_path}{Color.RESET}")
        return data_path
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(hash(task_id) % 2**32)  # æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹éšæœºç§å­
    data = {
        "timestamp": pd.date_range(start="2026-01-01", periods=config["data_rows"], freq="1min"),
        "metabolic_efficiency": np.random.uniform(0.6, 0.95, size=config["data_rows"]),
        "domain_adapt_score": np.random.uniform(0.5, 0.9, size=config["data_rows"]),
        "core_factor": np.random.uniform(0.7, 0.9, size=config["data_rows"]),
        "stability": np.random.uniform(0.65, 0.85, size=config["data_rows"]),
        "sample_id": [f"{task_id}_{i:04d}" for i in range(config["data_rows"])]
    }
    
    df = pd.DataFrame(data)
    df.to_csv(data_path, index=False, encoding="utf-8")
    logger.info(f"{Color.GREEN}âœ… ä»»åŠ¡{task_id}ç”Ÿæˆæ•°æ®ï¼š{data_path}ï¼ˆ{config['data_rows']}è¡Œï¼‰{Color.RESET}")
    return data_path

def get_task_logger(task_id: str):
    """è·å–å¸¦ä»»åŠ¡IDçš„logger"""
    task_logger = logging.getLogger(f"BatchProcess-{task_id}")
    def log(msg, level="info"):
        if level == "info":
            task_logger.info(msg, extra={"task_id": task_id})
        elif level == "error":
            task_logger.error(msg, extra={"task_id": task_id})
        elif level == "warning":
            task_logger.warning(msg, extra={"task_id": task_id})
    return log

# ------------------------------ å•ä¸ªä»»åŠ¡å¤„ç†å‡½æ•° ------------------------------
def process_single_task(task: dict, config: dict) -> dict:
    """å¤„ç†å•ä¸ªä»»åŠ¡ï¼ˆæ”¯æŒä¸åŒä»»åŠ¡ç±»å‹ï¼‰"""
    task_id = task["task_id"]
    log = get_task_logger(task_id)
    task_result = {
        "task_id": task_id,
        "config": task,
        "status": "failed",
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "elapsed_time": 0,
        "metrics": {},
        "error": None
    }
    
    try:
        # 1. å‡†å¤‡ä»»åŠ¡ç›®å½•å’Œæ•°æ®
        task_dir = f"{config['output_dir']}/{task_id}"
        ensure_dir(task_dir)
        data_path = generate_task_data(task_id, config) if config["generate_data"] else task.get("data_path")
        
        if not data_path or not os.path.exists(data_path):
            raise ValueError(f"ä»»åŠ¡{task_id}æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")
        
        log(f"{Color.CYAN}ğŸš€ å¼€å§‹å¤„ç†ä»»åŠ¡ï¼ˆç±»å‹ï¼š{config['task_type']}ï¼‰{Color.RESET}")
        start_time = time.time()
        
        # 2. æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒæ“ä½œ
        if config["task_type"] == "run" or config["task_type"] == "all":
            # æ‰§è¡Œæ™ºèƒ½ä½“è¿è¡Œ
            log(f"{Color.BLUE}ğŸ”„ æ‰§è¡Œæ™ºèƒ½ä½“è¿è¡Œï¼ˆé¢†åŸŸï¼š{task['domain']}ï¼‰{Color.RESET}")
            cmd = UniversalCmd()
            run_output = f"{task_dir}/{task_id}_run_result.csv"
            
            run_args = type('Args', (object,), {
                "data_path": data_path,
                "domain": task["domain"],
                "run_time": task.get("run_time", 60),
                "output_path": run_output
            })
            
            run_res = cmd._execute_run(run_args, return_result=True)
            task_result["metrics"]["run"] = {
                "avg_metabolic_efficiency": run_res["core_metrics"]["avg_metabolic_efficiency"],
                "domain_adapt_score": run_res["core_metrics"]["domain_adapt_score"],
                "stability_score": run_res["core_metrics"]["stability_score"],
                "output_path": run_output
            }
            log(f"{Color.GREEN}âœ… è¿è¡Œå®Œæˆï¼šå¹³å‡ä»£è°¢æ•ˆç‡={run_res['core_metrics']['avg_metabolic_efficiency']:.3f}{Color.RESET}")
        
        if config["task_type"] == "tune" or config["task_type"] == "all":
            # æ‰§è¡Œæ™ºèƒ½ä½“è°ƒä¼˜
            log(f"{Color.BLUE}ğŸ”§ æ‰§è¡Œæ™ºèƒ½ä½“è°ƒä¼˜ï¼ˆè¿­ä»£ï¼š{task['iter']}ï¼Œå­¦ä¹ ç‡ï¼š{task['lr']}ï¼‰{Color.RESET}")
            tuner = TunerDashboard()
            tuner.default_params.update({
                "domain": task["domain"],
                "adapt_iterations": task["iter"],
                "learning_rate": task["lr"],
                "target_metric": "metabolic_efficiency"
            })
            
            tuner._start_tuner(data_path)
            # ç­‰å¾…è°ƒä¼˜å®Œæˆ
            while tuner.tuner_status["is_running"]:
                time.sleep(0.5)
            
            # è®°å½•è°ƒä¼˜ç»“æœ
            tune_result = {
                "best_score": tuner.tuner_status["best_score"],
                "convergence_iter": tuner.tuner_status["convergence_iter"],
                "stability_score": tuner.tuner_status["stability_score"],
                "best_params": tuner.tuner_status["best_params"]
            }
            task_result["metrics"]["tune"] = tune_result
            log(f"{Color.GREEN}âœ… è°ƒä¼˜å®Œæˆï¼šæœ€ä¼˜å¾—åˆ†={tune_result['best_score']:.3f}{Color.RESET}")
            
            # ä¿å­˜è°ƒä¼˜ç»“æœ
            tune_output = f"{task_dir}/{task_id}_tune_result.json"
            with open(tune_output, "w", encoding="utf-8") as f:
                json.dump(tune_result, f, indent=2)
            task_result["metrics"]["tune"]["output_path"] = tune_output
        
        if config["task_type"] == "analyze" or config["task_type"] == "all":
            # æ‰§è¡Œç»“æœåˆ†æ
            log(f"{Color.BLUE}ğŸ“Š æ‰§è¡Œç»“æœåˆ†æ{Color.RESET}")
            analyzer = ResultAnalyzer(output_dir=f"{task_dir}/analysis")
            analysis_data = {
                "task_id": task_id,
                "metrics": task_result["metrics"],
                "analysis_time": datetime.now().isoformat()
            }
            
            # ä¿å­˜åˆ†æç»“æœ
            analysis_output = f"{task_dir}/{task_id}_analysis.json"
            with open(analysis_output, "w", encoding="utf-8") as f:
                json.dump(analysis_data, f, indent=2)
            task_result["metrics"]["analysis"] = {"output_path": analysis_output}
            log(f"{Color.GREEN}âœ… åˆ†æå®Œæˆï¼šç»“æœä¿å­˜è‡³{analysis_output}{Color.RESET}")
        
        # 3. ä»»åŠ¡å®Œæˆå¤„ç†
        elapsed_time = time.time() - start_time
        task_result.update({
            "status": "success",
            "end_time": datetime.now().isoformat(),
            "elapsed_time": round(elapsed_time, 2)
        })
        
        log(f"{Color.GREEN}ğŸ‰ ä»»åŠ¡å¤„ç†å®Œæˆï¼ˆè€—æ—¶ï¼š{elapsed_time:.2f}ç§’ï¼‰{Color.RESET}")
        
    except Exception as e:
        error_msg = str(e)
        task_result["error"] = error_msg
        log(f"{Color.RED}âŒ ä»»åŠ¡å¤„ç†å¤±è´¥ï¼š{error_msg}{Color.RESET}", level="error")
    
    return task_result

# ------------------------------ æ‰¹é‡ä»»åŠ¡è°ƒåº¦ ------------------------------
def run_batch_process(config: dict):
    """æ‰§è¡Œæ‰¹é‡å¤„ç†"""
    # åˆå§‹åŒ–è¾“å‡ºç›®å½•
    ensure_dir(config["output_dir"])
    batch_id = f"batch_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    logger.info(f"{Color.PURPLE}ğŸ“¦ å¯åŠ¨æ‰¹é‡å¤„ç†ï¼ˆæ‰¹æ¬¡IDï¼š{batch_id}ï¼‰{Color.RESET}")
    logger.info(f"{Color.BLUE}ğŸ“‹ æ‰¹é‡é…ç½®ï¼šä»»åŠ¡æ•°={len(config['tasks'])} | å¹¶è¡Œæ•°={config['parallel_workers']} | ä»»åŠ¡ç±»å‹={config['task_type']}{Color.RESET}")
    
    # å­˜å‚¨æ‰¹é‡ç»“æœ
    batch_result = {
        "batch_id": batch_id,
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "total_tasks": len(config["tasks"]),
        "success_tasks": 0,
        "failed_tasks": 0,
        "task_results": [],
        "summary": {}
    }
    
    try:
        # 1. æ‰§è¡Œæ‰¹é‡ä»»åŠ¡
        if config["parallel_workers"] > 1:
            # å¹¶è¡Œæ‰§è¡Œ
            logger.info(f"{Color.BLUE}âš¡ é‡‡ç”¨å¹¶è¡Œæ¨¡å¼æ‰§è¡Œä»»åŠ¡ï¼ˆ{config['parallel_workers']}çº¿ç¨‹ï¼‰{Color.RESET}")
            with ThreadPoolExecutor(max_workers=config["parallel_workers"]) as executor:
                futures = {executor.submit(process_single_task, task, config): task for task in config["tasks"]}
                
                for future in as_completed(futures):
                    task_res = future.result()
                    batch_result["task_results"].append(task_res)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    if task_res["status"] == "failed":
                        batch_result["failed_tasks"] += 1
                    else:
                        batch_result["success_tasks"] += 1
                    
                    # æ‰“å°è¿›åº¦
                    completed = len(batch_result["task_results"])
                    progress = (completed / config["tasks"]) * 100
                    logger.info(f"{Color.YELLOW}ğŸ“Š è¿›åº¦ï¼š{completed}/{len(config['tasks'])} ({progress:.1f}%) | æˆåŠŸï¼š{batch_result['success_tasks']} | å¤±è´¥ï¼š{batch_result['failed_tasks']}{Color.RESET}")
        else:
            # ä¸²è¡Œæ‰§è¡Œ
            logger.info(f"{Color.BLUE}ğŸ“¶ é‡‡ç”¨ä¸²è¡Œæ¨¡å¼æ‰§è¡Œä»»åŠ¡{Color.RESET}")
            for task in config["tasks"]:
                task_res = process_single_task(task, config)
                batch_result["task_results"].append(task_res)
                
                if task_res["status"] == "failed":
                    batch_result["failed_tasks"] += 1
                else:
                    batch_result["success_tasks"] += 1
                
                completed = len(batch_result["task_results"])
                progress = (completed / len(config["tasks"])) * 100
                logger.info(f"{Color.YELLOW}ğŸ“Š è¿›åº¦ï¼š{completed}/{len(config['tasks'])} ({progress:.1f}%) | æˆåŠŸï¼š{batch_result['success_tasks']} | å¤±è´¥ï¼š{batch_result['failed_tasks']}{Color.RESET}")
        
        # 2. ç”Ÿæˆæ‰¹é‡æ±‡æ€»
        if config["save_batch_summary"]:
            logger.info(f"{Color.BLUE}ğŸ“ˆ ç”Ÿæˆæ‰¹é‡æ±‡æ€»æŠ¥å‘Š{Color.RESET}")
            # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
            success_rate = (batch_result["success_tasks"] / batch_result["total_tasks"]) * 100 if batch_result["total_tasks"] > 0 else 0
            avg_elapsed = 0
            domain_metrics = {}
            
            for task_res in batch_result["task_results"]:
                if task_res["status"] != "failed":
                    avg_elapsed += task_res["elapsed_time"]
                    domain = task_res["config"]["domain"]
                    if domain not in domain_metrics:
                        domain_metrics[domain] = {"count": 0, "avg_score": 0}
                    
                    # æ±‡æ€»è°ƒä¼˜/è¿è¡ŒæŒ‡æ ‡
                    if "tune" in task_res["metrics"]:
                        domain_metrics[domain]["avg_score"] += task_res["metrics"]["tune"]["best_score"]
                    elif "run" in task_res["metrics"]:
                        domain_metrics[domain]["avg_score"] += task_res["metrics"]["run"]["avg_metabolic_efficiency"]
                    domain_metrics[domain]["count"] += 1
            
            # è®¡ç®—é¢†åŸŸå¹³å‡å¾—åˆ†
            for domain in domain_metrics:
                if domain_metrics[domain]["count"] > 0:
                    domain_metrics[domain]["avg_score"] /= domain_metrics[domain]["count"]
            
            avg_elapsed = avg_elapsed / batch_result["success_tasks"] if batch_result["success_tasks"] > 0 else 0
            
            # æ„å»ºæ±‡æ€»æ•°æ®
            batch_result["summary"] = {
                "success_rate": round(success_rate, 2),
                "avg_elapsed_time": round(avg_elapsed, 2),
                "domain_metrics": domain_metrics,
                "top_task": None
            }
            
            # æ‰¾å‡ºæœ€ä¼˜ä»»åŠ¡
            top_score = 0
            top_task = None
            for task_res in batch_result["task_results"]:
                if task_res["status"] == "success":
                    if "tune" in task_res["metrics"]:
                        score = task_res["metrics"]["tune"]["best_score"]
                    else:
                        score = task_res["metrics"]["run"]["avg_metabolic_efficiency"]
                    
                    if score > top_score:
                        top_score = score
                        top_task = task_res["task_id"]
            
            batch_result["summary"]["top_task"] = top_task
            batch_result["summary"]["top_score"] = round(top_score, 3) if top_task else 0
            
            # ä¿å­˜æ±‡æ€»æ–‡ä»¶
            summary_path = f"{config['output_dir']}/batch_summary_{batch_id}.json"
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(batch_result["summary"], f, ensure_ascii=False, indent=2)
            logger.info(f"{Color.GREEN}âœ… æ‰¹é‡æ±‡æ€»å·²ä¿å­˜ï¼š{summary_path}{Color.RESET}")
        
        # 3. å®Œæˆæ‰¹é‡å¤„ç†
        batch_result["end_time"] = datetime.now().isoformat()
        total_elapsed = (datetime.fromisoformat(batch_result["end_time"]) - 
                         datetime.fromisoformat(batch_result["start_time"])).total_seconds()
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        result_path = f"{config['output_dir']}/batch_result_{batch_id}.json"
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(batch_result, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‰¹é‡ç»“æœ
        logger.info(f"{Color.PURPLE}========== æ‰¹é‡å¤„ç†å®Œæˆ =========={Color.RESET}")
        logger.info(f"{Color.GREEN}ğŸ“Š æ‰¹é‡ç»“æœï¼š{Color.RESET}")
        logger.info(f"   æ€»ä»»åŠ¡æ•°ï¼š{batch_result['total_tasks']}")
        logger.info(f"   æˆåŠŸæ•°ï¼š{batch_result['success_tasks']} | å¤±è´¥æ•°ï¼š{batch_result['failed_tasks']}")
        logger.info(f"   æˆåŠŸç‡ï¼š{success_rate:.2f}%")
        logger.info(f"   å¹³å‡è€—æ—¶ï¼š{avg_elapsed:.2f}ç§’/ä»»åŠ¡")
        logger.info(f"   æ€»è€—æ—¶ï¼š{total_elapsed:.2f}ç§’")
        logger.info(f"   æœ€ä¼˜ä»»åŠ¡ï¼š{top_task}ï¼ˆå¾—åˆ†ï¼š{top_score:.3f}ï¼‰")
        logger.info(f"   ç»“æœæ–‡ä»¶ï¼š{result_path}")
        logger.info(f"{Color.PURPLE}=================================={Color.RESET}")
        
    except Exception as e:
        logger.error(f"{Color.RED}âŒ æ‰¹é‡å¤„ç†å¤±è´¥ï¼š{e}{Color.RESET}")
        batch_result["error"] = str(e)
    
    return batch_result

# ------------------------------ å‘½ä»¤è¡Œå…¥å£ ------------------------------
def main():
    """æ‰¹é‡å¤„ç†ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="UMC-Metabolic-Agent é€šç”¨æ‰¹é‡å¤„ç†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
{Color.CYAN}ä½¿ç”¨ç¤ºä¾‹ï¼š{Color.RESET}
  1. é»˜è®¤é…ç½®è¿è¡Œï¼š
     python batch_process.py
     
  2. ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼š
     python batch_process.py --config my_config.json
     
  3. æŒ‡å®šä»»åŠ¡ç±»å‹å’Œå¹¶è¡Œæ•°ï¼š
     python batch_process.py --task-type run --workers 4
     
  4. ä»…è°ƒä¼˜ä»»åŠ¡ï¼Œè¦†ç›–å·²æœ‰ç»“æœï¼š
     python batch_process.py --task-type tune --overwrite --workers 2
     
  5. è‡ªå®šä¹‰è¾“å‡ºç›®å½•ï¼š
     python batch_process.py --output ./my_batch_output --task-type all
     
{Color.CYAN}é…ç½®æ–‡ä»¶æ ¼å¼ï¼ˆJSONï¼‰ï¼š{Color.RESET}
{{
  "output_dir": "./my_output",
  "parallel_workers": 2,
  "task_type": "tune",
  "tasks": [
    {{"task_id": "task1", "domain": "quantum", "iter": 50, "lr": 0.01}},
    {{"task_id": "task2", "domain": "biology", "iter": 40, "lr": 0.008}}
  ]
}}
        """
    )
    
    # å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument("--config", "-c", type=str, help="è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰")
    parser.add_argument("--task-type", "-t", type=str, choices=["run", "tune", "analyze", "all"],
                        help="ä»»åŠ¡ç±»å‹ï¼šrun(ä»…è¿è¡Œ)/tune(ä»…è°ƒä¼˜)/analyze(ä»…åˆ†æ)/all(å…¨æµç¨‹)")
    parser.add_argument("--workers", "-w", type=int, help="å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--overwrite", action="store_true", help="è¦†ç›–å·²æœ‰ç»“æœæ–‡ä»¶")
    parser.add_argument("--no-data", action="store_false", dest="generate_data", help="ä¸è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®")
    parser.add_argument("--no-summary", action="store_false", dest="save_batch_summary", help="ä¸ç”Ÿæˆæ‰¹é‡æ±‡æ€»")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®å¹¶è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
    config = load_config(args.config)
    if args.task_type:
        config["task_type"] = args.task_type
    if args.workers:
        config["parallel_workers"] = args.workers
    if args.output:
        config["output_dir"] = args.output
    if args.overwrite:
        config["overwrite"] = args.overwrite
    if args.generate_data is not None:
        config["generate_data"] = args.generate_data
    if args.save_batch_summary is not None:
        config["save_batch_summary"] = args.save_batch_summary
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    run_batch_process(config)

if __name__ == "__main__":
    main()