# -*- coding: utf-8 -*-
"""
UMC-Metabolic-Agent å¤šæ¨¡æ€æ•°æ®è§£ææ¨¡å—ï¼ˆæ–‡æœ¬/è¡¨æ ¼/å›¾ç‰‡/æ—¶åº/JSONç»Ÿä¸€è§£æ+æ ‡å‡†åŒ–ï¼‰
æ ¸å¿ƒé€»è¾‘ï¼šé€‚é…å¤šæºå¼‚æ„æ•°æ®ï¼Œç»Ÿä¸€è§£æä¸ºç»“æ„åŒ–æ•°å€¼æ•°æ®ï¼Œä¾›UMCæ™ºèƒ½ä½“ç›´æ¥ä½¿ç”¨
è®¾è®¡åŸåˆ™ï¼šæ¨¡æ€ä¸“å±è§£æã€è·¨æ¨¡æ€å¯¹é½ã€æ™ºèƒ½è¡¥å…¨ã€é›¶é…ç½®ä½¿ç”¨ï¼Œé€‚é…æ–°æ‰‹å¤„ç†å¤šæ¨¡æ€æ•°æ®
"""
import pandas as pd
import numpy as np
import json
import os
import re
import cv2
import base64
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple, Union
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

# å¯¼å…¥æ ¸å¿ƒå·¥å…·
from tool_build import create_test_data

class MultimodalParser:
    """å¤šæ¨¡æ€æ•°æ®è§£æå™¨ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼šå¤šç±»å‹æ•°æ®è§£æã€æ ‡å‡†åŒ–ã€èåˆã€è¡¥å…¨ï¼‰"""
    def __init__(self, output_dir: str = "./multimodal_processed"):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€è§£æå™¨
        :param output_dir: è§£æåæ•°æ®çš„ä¿å­˜ç›®å½•
        """
        # åŸºç¡€é…ç½®
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        # æ”¯æŒçš„æ¨¡æ€ç±»å‹
        self.supported_modalities = ["table", "text", "image", "timeseries", "json"]
        # è§£æå†å²
        self.parse_history = []
        # æ ‡å‡†åŒ–å™¨å’Œè¡¥å…¨å™¨ï¼ˆå¤ç”¨é¿å…é‡å¤è®­ç»ƒï¼‰
        self.scaler = StandardScaler()
        self.imputer = KNNImputer(n_neighbors=5)

    def parse_table(self, data_source: Union[str, pd.DataFrame], **kwargs) -> pd.DataFrame:
        """
        è¡¨æ ¼æ•°æ®è§£æï¼ˆæ”¯æŒCSV/Excel/JSONæ–‡ä»¶æˆ–DataFrameï¼Œæ ¸å¿ƒæ¨¡æ€ï¼‰
        :param data_source: æ–‡ä»¶è·¯å¾„æˆ–DataFrame
        :param kwargs: å¯é€‰å‚æ•°ï¼ˆheader=0, index_col=None, sheet_name=0ç­‰ï¼‰
        :return: æ ‡å‡†åŒ–åçš„è¡¨æ ¼æ•°æ®
        """
        print("\nğŸ“Š å¼€å§‹è¡¨æ ¼æ•°æ®è§£æ...")
        # 1. åŠ è½½æ•°æ®
        if isinstance(data_source, str):
            if data_source.endswith(".csv"):
                df = pd.read_csv(data_source, header=kwargs.get("header", 0), index_col=kwargs.get("index_col", None), encoding="utf-8")
            elif data_source.endswith((".xlsx", ".xls")):
                df = pd.read_excel(data_source, sheet_name=kwargs.get("sheet_name", 0), header=kwargs.get("header", 0), index_col=kwargs.get("index_col", None))
            elif data_source.endswith(".json"):
                df = pd.read_json(data_source, encoding="utf-8")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¡¨æ ¼æ–‡ä»¶æ ¼å¼ï¼š{data_source}")
        elif isinstance(data_source, pd.DataFrame):
            df = data_source.copy()
        else:
            raise TypeError(f"è¡¨æ ¼æ•°æ®æºç±»å‹ä¸æ”¯æŒï¼š{type(data_source)}")

        # 2. æ•°æ®æ¸…æ´—ï¼ˆæ ¸å¿ƒï¼šé€‚é…UMCæ™ºèƒ½ä½“è¾“å…¥è¦æ±‚ï¼‰
        parsed_df = self._clean_and_standardize(df, "table")
        print(f"âœ… è¡¨æ ¼æ•°æ®è§£æå®Œæˆï¼š{len(parsed_df)}è¡Œ Ã— {len(parsed_df.columns)}åˆ—")
        return parsed_df

    def parse_text(self, data_source: Union[str, List[str]], text_type: str = "numeric_extract", **kwargs) -> pd.DataFrame:
        """
        æ–‡æœ¬æ•°æ®è§£æï¼ˆæ”¯æŒæ–‡æœ¬æ–‡ä»¶/æ–‡æœ¬åˆ—è¡¨ï¼Œæå–æ•°å€¼ç‰¹å¾ï¼‰
        :param data_source: æ–‡æœ¬æ–‡ä»¶è·¯å¾„æˆ–æ–‡æœ¬åˆ—è¡¨
        :param text_type: è§£æç±»å‹ï¼ˆnumeric_extractï¼šæ•°å€¼æå–/keyword_extractï¼šå…³é”®è¯æ•°å€¼åŒ–ï¼‰
        :param kwargs: å¯é€‰å‚æ•°ï¼ˆtarget_colsï¼šç›®æ ‡åˆ—ååˆ—è¡¨ï¼‰
        :return: æ ‡å‡†åŒ–åçš„æ•°å€¼è¡¨æ ¼æ•°æ®
        """
        print("\nğŸ“ å¼€å§‹æ–‡æœ¬æ•°æ®è§£æ...")
        # 1. åŠ è½½æ–‡æœ¬
        if isinstance(data_source, str):
            with open(data_source, "r", encoding="utf-8") as f:
                texts = [line.strip() for line in f if line.strip()]
        elif isinstance(data_source, list) and all(isinstance(t, str) for t in data_source):
            texts = data_source
        else:
            raise TypeError(f"æ–‡æœ¬æ–‡æºç±»å‹ä¸æ”¯æŒï¼š{type(data_source)}")

        # 2. æ–‡æœ¬è§£æï¼ˆæå–æ•°å€¼ç‰¹å¾ï¼‰
        parsed_data = []
        target_cols = kwargs.get("target_cols", ["feature_1", "feature_2", "feature_3"])

        if text_type == "numeric_extract":
            # æå–æ–‡æœ¬ä¸­çš„æ‰€æœ‰æ•°å€¼ï¼ˆé€‚é…å®éªŒæŠ¥å‘Š/ç›‘æµ‹æ—¥å¿—ç­‰æ–‡æœ¬ï¼‰
            for text in texts:
                # æ­£åˆ™æå–æµ®ç‚¹æ•°/æ•´æ•°
                nums = re.findall(r'-?\d+\.?\d*', text)
                nums = [float(num) for num in nums] if nums else [0.0]*len(target_cols)
                # å¯¹é½åˆ—æ•°ï¼ˆä¸è¶³è¡¥0ï¼Œè¶…è¿‡æˆªæ–­ï¼‰
                nums = nums[:len(target_cols)] if len(nums) > len(target_cols) else nums + [0.0]*(len(target_cols)-len(nums))
                parsed_data.append(nums)

        elif text_type == "keyword_extract":
            # å…³é”®è¯æ•°å€¼åŒ–ï¼ˆé€‚é…é¢†åŸŸæè¿°æ–‡æœ¬ï¼‰
            # å†…ç½®é¢†åŸŸå…³é”®è¯åº“
            domain_keywords = {
                "quantum": ["qubit", "é‡å­", "ç¨³å®šæ€§", "èƒ½è€—", "ç‰©è´¨è¾“å‡º"],
                "atomic": ["åŸå­", "é¢‘ç‡", "èƒ½æ•ˆ", "ç²’å­äº§ç‡"],
                "logistics": ["ç‰©æµ", "æ•ˆç‡", "æˆæœ¬", "é€Ÿåº¦"]
            }
            for text in texts:
                # è®¡ç®—æ–‡æœ¬ä¸å„é¢†åŸŸå…³é”®è¯çš„åŒ¹é…åº¦
                keyword_scores = []
                for domain, keywords in domain_keywords.items():
                    score = sum([1 for kw in keywords if kw in text]) / len(keywords)
                    keyword_scores.append(score)
                # å¯¹é½åˆ—æ•°
                keyword_scores = keyword_scores[:len(target_cols)] if len(keyword_scores) > len(target_cols) else keyword_scores + [0.0]*(len(target_cols)-len(keyword_scores))
                parsed_data.append(keyword_scores)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æœ¬è§£æç±»å‹ï¼š{text_type}")

        # 3. è½¬ä¸ºDataFrameå¹¶æ ‡å‡†åŒ–
        df = pd.DataFrame(parsed_data, columns=target_cols)
        parsed_df = self._clean_and_standardize(df, "text")
        print(f"âœ… æ–‡æœ¬æ•°æ®è§£æå®Œæˆï¼š{len(parsed_df)}è¡Œ Ã— {len(parsed_df.columns)}åˆ—")
        return parsed_df

    def parse_image(self, data_source: Union[str, List[str]], extract_type: str = "pixel_stat", **kwargs) -> pd.DataFrame:
        """
        å›¾ç‰‡æ•°æ®è§£æï¼ˆæ”¯æŒå›¾ç‰‡æ–‡ä»¶/è·¯å¾„åˆ—è¡¨ï¼Œæ•°å€¼åŒ–æå–è§†è§‰ç‰¹å¾ï¼‰
        :param data_source: å›¾ç‰‡æ–‡ä»¶è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
        :param extract_type: ç‰¹å¾æå–ç±»å‹ï¼ˆpixel_statï¼šåƒç´ ç»Ÿè®¡/edge_densityï¼šè¾¹ç¼˜å¯†åº¦ï¼‰
        :param kwargs: å¯é€‰å‚æ•°ï¼ˆtarget_colsï¼šç›®æ ‡åˆ—ååˆ—è¡¨ï¼‰
        :return: æ ‡å‡†åŒ–åçš„æ•°å€¼è¡¨æ ¼æ•°æ®
        """
        print("\nğŸ–¼ï¸ å¼€å§‹å›¾ç‰‡æ•°æ®è§£æ...")
        # æ£€æŸ¥OpenCVä¾èµ–
        try:
            import cv2
        except ImportError:
            raise ImportError("è§£æå›¾ç‰‡éœ€è¦å®‰è£…OpenCVï¼špip install opencv-python")

        # 1. åŠ è½½å›¾ç‰‡è·¯å¾„
        if isinstance(data_source, str):
            if os.path.isdir(data_source):
                # ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡
                img_ext = [".jpg", ".jpeg", ".png", ".bmp"]
                img_paths = [os.path.join(data_source, f) for f in os.listdir(data_source) if f.lower().endswith(tuple(img_ext))]
            elif os.path.isfile(data_source) and data_source.lower().endswith(tuple([".jpg", ".jpeg", ".png", ".bmp"])):
                img_paths = [data_source]
            else:
                raise ValueError(f"å›¾ç‰‡æºæ— æ•ˆï¼š{data_source}")
        elif isinstance(data_source, list) and all(isinstance(p, str) for p in data_source):
            img_paths = data_source
        else:
            raise TypeError(f"å›¾ç‰‡æºç±»å‹ä¸æ”¯æŒï¼š{type(data_source)}")

        # 2. å›¾ç‰‡ç‰¹å¾æå–ï¼ˆæ•°å€¼åŒ–ï¼‰
        parsed_data = []
        target_cols = kwargs.get("target_cols", ["mean_brightness", "contrast", "edge_density"])

        for img_path in img_paths:
            try:
                # è¯»å–å›¾ç‰‡ï¼ˆç°åº¦å›¾ç®€åŒ–å¤„ç†ï¼‰
                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                if img is None:
                    print(f"âš ï¸  æ— æ³•è¯»å–å›¾ç‰‡ï¼š{img_path}ï¼Œè·³è¿‡")
                    continue

                # ç‰¹å¾æå–
                if extract_type == "pixel_stat":
                    # åƒç´ ç»Ÿè®¡ç‰¹å¾ï¼ˆäº®åº¦å‡å€¼ã€å¯¹æ¯”åº¦ã€ç†µï¼‰
                    mean_bright = np.mean(img) / 255.0  # å½’ä¸€åŒ–åˆ°0~1
                    contrast = np.std(img) / 255.0
                    entropy = cv2.calcHist([img], [0], None, [256], [0, 256]).flatten()
                    entropy = -np.sum(entropy * np.log2(entropy + 1e-8)) / np.log2(256)  # å½’ä¸€åŒ–ç†µ
                    features = [mean_bright, contrast, entropy]

                elif extract_type == "edge_density":
                    # è¾¹ç¼˜å¯†åº¦ç‰¹å¾ï¼ˆCannyè¾¹ç¼˜æ£€æµ‹ï¼‰
                    edges = cv2.Canny(img, 100, 200)
                    edge_density = np.sum(edges) / (img.shape[0] * img.shape[1])  # è¾¹ç¼˜åƒç´ å æ¯”
                    # è¡¥å……å…¶ä»–ç‰¹å¾
                    mean_bright = np.mean(img) / 255.0
                    contrast = np.std(img) / 255.0
                    features = [edge_density, mean_bright, contrast]

                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡æå–ç±»å‹ï¼š{extract_type}")

                # å¯¹é½åˆ—æ•°
                features = features[:len(target_cols)] if len(features) > len(target_cols) else features + [0.0]*(len(target_cols)-len(features))
                parsed_data.append(features)

            except Exception as e:
                print(f"âš ï¸  å¤„ç†å›¾ç‰‡{img_path}å¤±è´¥ï¼š{str(e)}ï¼Œå¡«å……é»˜è®¤å€¼")
                parsed_data.append([0.0]*len(target_cols))

        # 3. è½¬ä¸ºDataFrameå¹¶æ ‡å‡†åŒ–
        df = pd.DataFrame(parsed_data, columns=target_cols)
        parsed_df = self._clean_and_standardize(df, "image")
        print(f"âœ… å›¾ç‰‡æ•°æ®è§£æå®Œæˆï¼š{len(parsed_df)}è¡Œ Ã— {len(parsed_df.columns)}åˆ—")
        return parsed_df

    def parse_timeseries(self, data_source: Union[str, pd.DataFrame], **kwargs) -> pd.DataFrame:
        """
        æ—¶åºæ•°æ®è§£æï¼ˆæ”¯æŒCSV/Excel/JSONæˆ–DataFrameï¼Œæå–æ—¶åºç‰¹å¾ï¼‰
        :param data_source: æ–‡ä»¶è·¯å¾„æˆ–DataFrameï¼ˆéœ€åŒ…å«æ—¶é—´åˆ—å’Œæ•°å€¼åˆ—ï¼‰
        :param kwargs: å¯é€‰å‚æ•°ï¼ˆtime_colï¼šæ—¶é—´åˆ—åï¼Œwindow_sizeï¼šæ»‘åŠ¨çª—å£å¤§å°ï¼‰
        :return: æ ‡å‡†åŒ–åçš„æ—¶åºç‰¹å¾è¡¨æ ¼æ•°æ®
        """
        print("\nâ±ï¸ å¼€å§‹æ—¶åºæ•°æ®è§£æ...")
        # 1. åŠ è½½æ—¶åºæ•°æ®
        if isinstance(data_source, str):
            if data_source.endswith(".csv"):
                df = pd.read_csv(data_source, encoding="utf-8")
            elif data_source.endswith((".xlsx", ".xls")):
                df = pd.read_excel(data_source, encoding="utf-8")
            elif data_source.endswith(".json"):
                df = pd.read_json(data_source, encoding="utf-8")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ—¶åºæ–‡ä»¶æ ¼å¼ï¼š{data_source}")
        elif isinstance(data_source, pd.DataFrame):
            df = data_source.copy()
        else:
            raise TypeError(f"æ—¶åºæ•°æ®æºç±»å‹ä¸æ”¯æŒï¼š{type(data_source)}")

        # 2. æ—¶åºç‰¹å¾æå–
        time_col = kwargs.get("time_col", "timestamp")
        window_size = kwargs.get("window_size", 5)
        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
        df[time_col] = pd.to_datetime(df[time_col], errors="coerce")
        df = df.dropna(subset=[time_col])
        # æŒ‰æ—¶é—´æ’åº
        df = df.sort_values(by=time_col)

        # æå–æ•°å€¼åˆ—ï¼ˆæ’é™¤æ—¶é—´åˆ—ï¼‰
        numeric_cols = [col for col in df.columns if col != time_col and pd.api.types.is_numeric_dtype(df[col])]
        if not numeric_cols:
            raise ValueError("æ—¶åºæ•°æ®ä¸­æ— æ•°å€¼åˆ—å¯æå–ç‰¹å¾")

        # æ»‘åŠ¨çª—å£æå–æ—¶åºç‰¹å¾ï¼ˆå‡å€¼ã€æ–¹å·®ã€è¶‹åŠ¿ã€å³°å€¼ï¼‰
        parsed_data = []
        target_cols = []
        for col in numeric_cols:
            target_cols.extend([f"{col}_mean", f"{col}_std", f"{col}_trend", f"{col}_peak"])

        # æ»‘åŠ¨çª—å£è®¡ç®—
        for i in range(window_size-1, len(df)):
            window = df.iloc[i-window_size+1:i+1]
            row_features = []
            for col in numeric_cols:
                window_vals = window[col].values
                # å‡å€¼
                mean_val = np.mean(window_vals)
                # æ–¹å·®
                std_val = np.std(window_vals)
                # è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰
                x = np.arange(window_size)
                trend = np.polyfit(x, window_vals, 1)[0] if window_size >= 2 else 0.0
                # å³°å€¼ï¼ˆçª—å£å†…æœ€å¤§å€¼ï¼‰
                peak_val = np.max(window_vals)
                # æ·»åŠ ç‰¹å¾
                row_features.extend([mean_val, std_val, trend, peak_val])
            parsed_data.append(row_features)

        # 3. è½¬ä¸ºDataFrameå¹¶æ ‡å‡†åŒ–
        df_parsed = pd.DataFrame(parsed_data, columns=target_cols)
        parsed_df = self._clean_and_standardize(df_parsed, "timeseries")
        print(f"âœ… æ—¶åºæ•°æ®è§£æå®Œæˆï¼š{len(parsed_df)}è¡Œ Ã— {len(parsed_df.columns)}åˆ—")
        return parsed_df

    def parse_json(self, data_source: Union[str, dict, List[dict]], **kwargs) -> pd.DataFrame:
        """
        åŠç»“æ„åŒ–JSONè§£æï¼ˆæ”¯æŒJSONæ–‡ä»¶/å­—å…¸/å­—å…¸åˆ—è¡¨ï¼Œæ‰å¹³åŒ–ä¸ºè¡¨æ ¼ï¼‰
        :param data_source: JSONæ–‡ä»¶è·¯å¾„/å­—å…¸/å­—å…¸åˆ—è¡¨
        :param kwargs: å¯é€‰å‚æ•°ï¼ˆflatten_depthï¼šæ‰å¹³åŒ–æ·±åº¦ï¼‰
        :return: æ ‡å‡†åŒ–åçš„è¡¨æ ¼æ•°æ®
        """
        print("\nğŸ”§ å¼€å§‹JSONæ•°æ®è§£æ...")
        # 1. åŠ è½½JSONæ•°æ®
        if isinstance(data_source, str):
            with open(data_source, "r", encoding="utf-8") as f:
                json_data = json.load(f)
        elif isinstance(data_source, (dict, list)):
            json_data = data_source
        else:
            raise TypeError(f"JSONæºç±»å‹ä¸æ”¯æŒï¼š{type(data_source)}")

        # 2. æ‰å¹³åŒ–JSONï¼ˆå¤„ç†åµŒå¥—ç»“æ„ï¼‰
        flatten_depth = kwargs.get("flatten_depth", 2)
        df = self._flatten_json(json_data, flatten_depth)

        # 3. æ ‡å‡†åŒ–å¤„ç†
        parsed_df = self._clean_and_standardize(df, "json")
        print(f"âœ… JSONæ•°æ®è§£æå®Œæˆï¼š{len(parsed_df)}è¡Œ Ã— {len(parsed_df.columns)}åˆ—")
        return parsed_df

    def _flatten_json(self, json_data: Union[dict, List[dict]], depth: int = 2, parent_key: str = "") -> pd.DataFrame:
        """
        æ‰å¹³åŒ–åµŒå¥—JSONï¼ˆæ ¸å¿ƒè¾…åŠ©æ–¹æ³•ï¼‰
        :param json_data: JSONæ•°æ®
        :param depth: æ‰å¹³åŒ–æ·±åº¦
        :param parent_key: çˆ¶é”®ï¼ˆé€’å½’ç”¨ï¼‰
        :return: æ‰å¹³åŒ–åçš„DataFrame
        """
        flat_data = []

        def _flatten(item: Any, current_depth: int, current_key: str):
            if current_depth > depth:
                return {current_key: str(item)} if current_key else {}
            if isinstance(item, dict):
                result = {}
                for k, v in item.items():
                    new_key = f"{current_key}_{k}" if current_key else k
                    result.update(_flatten(v, current_depth+1, new_key))
                return result
            elif isinstance(item, list):
                result_list = []
                for i, elem in enumerate(item):
                    new_key = f"{current_key}_{i}" if current_key else str(i)
                    result_list.append(_flatten(elem, current_depth+1, new_key))
                # åˆå¹¶åˆ—è¡¨é¡¹ä¸ºè¡Œ
                merged = {}
                for res in result_list:
                    merged.update(res)
                return merged
            else:
                return {current_key: item} if current_key else {}

        # å¤„ç†å•å­—å…¸æˆ–å­—å…¸åˆ—è¡¨
        if isinstance(json_data, dict):
            flat_data.append(_flatten(json_data, 1, parent_key))
        elif isinstance(json_data, list):
            for item in json_data:
                flat_item = _flatten(item, 1, parent_key)
                flat_data.append(flat_item)

        # è½¬ä¸ºDataFrame
        df = pd.DataFrame(flat_data)
        # è¿‡æ»¤éæ•°å€¼åˆ—ï¼ˆä¿ç•™å¯è½¬æ¢ä¸ºæ•°å€¼çš„åˆ—ï¼‰
        for col in df.columns:
            try:
                df[col] = pd.to_numeric(df[col], errors="ignore")
            except:
                pass
        # åªä¿ç•™æ•°å€¼åˆ—
        numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
        df = df[numeric_cols] if numeric_cols else pd.DataFrame()
        return df

    def _clean_and_standardize(self, df: pd.DataFrame, modality: str) -> pd.DataFrame:
        """
        é€šç”¨æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–ï¼ˆæ‰€æœ‰æ¨¡æ€çš„ç»Ÿä¸€å¤„ç†é€»è¾‘ï¼‰
        :param df: åŸå§‹è§£ææ•°æ®
        :param modality: æ¨¡æ€ç±»å‹
        :return: æ ‡å‡†åŒ–åçš„DataFrame
        """
        # 1. å»é‡
        df_clean = df.drop_duplicates()
        # 2. è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆ3ÏƒåŸåˆ™ï¼‰
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            mean = df_clean[col].mean()
            std = df_clean[col].std()
            df_clean = df_clean[(df_clean[col] >= mean - 3*std) & (df_clean[col] <= mean + 3*std)]
        # 3. ç¼ºå¤±å€¼è¡¥å…¨ï¼ˆKNNï¼‰
        if not df_clean.empty and len(numeric_cols) > 0:
            df_clean[numeric_cols] = self.imputer.fit_transform(df_clean[numeric_cols])
        # 4. æ ‡å‡†åŒ–ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰
        if not df_clean.empty and len(numeric_cols) > 0:
            df_clean[numeric_cols] = self.scaler.fit_transform(df_clean[numeric_cols])
            # æ˜ å°„åˆ°0~1åŒºé—´ï¼ˆé€‚é…UMCæ™ºèƒ½ä½“è¾“å…¥è¦æ±‚ï¼‰
            df_clean[numeric_cols] = (df_clean[numeric_cols] - df_clean[numeric_cols].min()) / (df_clean[numeric_cols].max() - df_clean[numeric_cols].min() + 1e-8)
        # 5. é‡ç½®ç´¢å¼•
        df_clean = df_clean.reset_index(drop=True)
        # 6. ä¿å­˜è§£æåçš„æ•°æ®
        save_path = os.path.join(self.output_dir, f"{modality}_parsed_{time.strftime('%Y%m%d%H%M%S')}.csv")
        df_clean.to_csv(save_path, index=False, encoding="utf-8")
        print(f"ğŸ“ è§£æåæ•°æ®å·²ä¿å­˜ï¼š{save_path}")
        return df_clean

    def fuse_multimodal_data(self, data_dict: Dict[str, pd.DataFrame], align_method: str = "sample_count") -> pd.DataFrame:
        """
        å¤šæ¨¡æ€æ•°æ®èåˆï¼ˆå°†ä¸åŒæ¨¡æ€è§£æåçš„æ•°æ®èåˆä¸ºç»Ÿä¸€è¡¨æ ¼ï¼‰
        :param data_dict: æ¨¡æ€-æ•°æ®å­—å…¸ï¼ˆå¦‚{"table": df1, "text": df2}ï¼‰
        :param align_method: å¯¹é½æ–¹æ³•ï¼ˆsample_countï¼šæŒ‰æ ·æœ¬æ•°æˆªæ–­/col_mergeï¼šåˆ—åˆå¹¶ï¼‰
        :return: èåˆåçš„æ ‡å‡†åŒ–æ•°æ®
        """
        print("\nğŸ”— å¼€å§‹å¤šæ¨¡æ€æ•°æ®èåˆ...")
        if not data_dict:
            raise ValueError("æ— å¤šæ¨¡æ€æ•°æ®å¯èåˆ")

        # 1. æ•°æ®å¯¹é½
        fused_df = None
        if align_method == "sample_count":
            # æŒ‰æœ€å°æ ·æœ¬æ•°æˆªæ–­æ‰€æœ‰æ•°æ®
            min_samples = min([len(df) for df in data_dict.values()])
            aligned_dfs = [df.iloc[:min_samples].reset_index(drop=True) for df in data_dict.values()]
            # åˆ—é‡å‘½åé¿å…å†²çª
            renamed_dfs = []
            for idx, (modality, df) in enumerate(data_dict.items()):
                df_aligned = df.iloc[:min_samples].reset_index(drop=True)
                df_renamed = df_aligned.rename(columns={col: f"{modality}_{col}" for col in df_aligned.columns})
                renamed_dfs.append(df_renamed)
            # æ¨ªå‘åˆå¹¶
            fused_df = pd.concat(renamed_dfs, axis=1)

        elif align_method == "col_merge":
            # åˆ—åˆå¹¶ï¼ˆè¦æ±‚æ‰€æœ‰æ•°æ®æ ·æœ¬æ•°ç›¸åŒï¼‰
            sample_counts = [len(df) for df in data_dict.values()]
            if len(set(sample_counts)) > 1:
                raise ValueError("col_mergeæ–¹æ³•è¦æ±‚æ‰€æœ‰æ¨¡æ€æ•°æ®æ ·æœ¬æ•°ç›¸åŒ")
            renamed_dfs = []
            for modality, df in data_dict.items():
                df_renamed = df.rename(columns={col: f"{modality}_{col}" for col in df.columns})
                renamed_dfs.append(df_renamed)
            fused_df = pd.concat(renamed_dfs, axis=1)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆå¯¹é½æ–¹æ³•ï¼š{align_method}")

        # 2. æ ‡å‡†åŒ–èåˆåçš„æ•°æ®
        fused_df = self._clean_and_standardize(fused_df, "multimodal_fused")
        print(f"âœ… å¤šæ¨¡æ€æ•°æ®èåˆå®Œæˆï¼š{len(fused_df)}è¡Œ Ã— {len(fused_df.columns)}åˆ—")
        return fused_df

    def run_multimodal_parse(self, parse_config: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """
        ä¸€é”®è¿è¡Œå¤šæ¨¡æ€è§£æï¼ˆæ”¯æŒæ‰¹é‡è§£æä¸åŒæ¨¡æ€æ•°æ®ï¼‰
        :param parse_config: è§£æé…ç½®å­—å…¸
        ç¤ºä¾‹ï¼š
        {
            "table": {"data_source": "./quantum_data.csv"},
            "text": {"data_source": ["./report.txt"], "text_type": "numeric_extract"},
            "image": {"data_source": "./img_dir", "extract_type": "edge_density"}
        }
        :return: æ¨¡æ€-è§£æåæ•°æ®å­—å…¸
        """
        print("\nğŸš€ å¼€å§‹å¤šæ¨¡æ€æ‰¹é‡è§£æ...")
        parse_results = {}
        # éå†é…ç½®è§£æå„æ¨¡æ€
        for modality, config in parse_config.items():
            if modality not in self.supported_modalities:
                print(f"âš ï¸  ä¸æ”¯æŒçš„æ¨¡æ€ç±»å‹ï¼š{modality}ï¼Œè·³è¿‡")
                continue
            try:
                if modality == "table":
                    df = self.parse_table(**config)
                elif modality == "text":
                    df = self.parse_text(**config)
                elif modality == "image":
                    df = self.parse_image(**config)
                elif modality == "timeseries":
                    df = self.parse_timeseries(**config)
                elif modality == "json":
                    df = self.parse_json(**config)
                parse_results[modality] = df
                # è®°å½•è§£æå†å²
                self.parse_history.append({
                    "modality": modality,
                    "config": config,
                    "sample_count": len(df),
                    "col_count": len(df.columns),
                    "parse_time": time.strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception as e:
                print(f"âŒ è§£æ{modality}æ¨¡æ€å¤±è´¥ï¼š{str(e)}")

        print(f"âœ… å¤šæ¨¡æ€æ‰¹é‡è§£æå®Œæˆï¼æˆåŠŸè§£æ{len(parse_results)}ç§æ¨¡æ€")
        return parse_results

# å¤šæ¨¡æ€è§£ææ¨¡å—éªŒè¯å…¥å£ï¼ˆä¸€ç«™å¼æµ‹è¯•ï¼‰
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–å¤šæ¨¡æ€è§£æå™¨
    parser = MultimodalParser()
    print("ğŸš€ å¤šæ¨¡æ€è§£æå™¨åˆå§‹åŒ–å®Œæˆï¼")

    # 2. ç”Ÿæˆæµ‹è¯•æ•°æ®
    # è¡¨æ ¼æµ‹è¯•æ•°æ®
    table_data = create_test_data(domain_name="quantum", sample_count=100)
    table_data_path = "./test_multimodal_table.csv"
    table_data.to_csv(table_data_path, index=False, encoding="utf-8")

    # æ–‡æœ¬æµ‹è¯•æ•°æ®ï¼ˆå®éªŒæŠ¥å‘Šæ–‡æœ¬ï¼‰
    text_data = [
        "é‡å­å®éªŒæŠ¥å‘Šï¼šqubitç¨³å®šæ€§0.85ï¼Œèƒ½è€—0.72ï¼Œç‰©è´¨è¾“å‡º0.68",
        "åŸå­å®éªŒæŠ¥å‘Šï¼šåŸå­é¢‘ç‡0.78ï¼Œèƒ½æ•ˆ0.65ï¼Œç²’å­äº§ç‡0.59",
        "ç‰©æµç›‘æµ‹ï¼šç‰©æµæ•ˆç‡0.82ï¼Œè¿è¾“æˆæœ¬0.75ï¼Œé…é€é€Ÿåº¦0.69"
    ]
    text_data_path = "./test_multimodal_text.txt"
    with open(text_data_path, "w", encoding="utf-8") as f:
        f.write("\n".join(text_data))

    # æ—¶åºæµ‹è¯•æ•°æ®
    timeseries_data = pd.DataFrame({
        "timestamp": pd.date_range(start="2026-01-01", periods=50, freq="H"),
        "qubit_stability": np.random.rand(50)*0.9,
        "energy_consumption": np.random.rand(50)*0.8
    })
    timeseries_data_path = "./test_multimodal_timeseries.csv"
    timeseries_data.to_csv(timeseries_data_path, index=False, encoding="utf-8")

    # JSONæµ‹è¯•æ•°æ®
    json_data = [
        {"quantum": {"qubit_stability": 0.85, "energy_consumption": 0.72}, "timestamp": "2026-01-01 00:00:00"},
        {"quantum": {"qubit_stability": 0.82, "energy_consumption": 0.75}, "timestamp": "2026-01-01 01:00:00"}
    ]
    json_data_path = "./test_multimodal_json.json"
    with open(json_data_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    # 3. ä¸€é”®å¤šæ¨¡æ€è§£æ
    parse_config = {
        "table": {"data_source": table_data_path},
        "text": {"data_source": text_data_path, "text_type": "numeric_extract", "target_cols": ["qubit_stability", "energy_consumption", "matter_output"]},
        "timeseries": {"data_source": timeseries_data_path, "time_col": "timestamp", "window_size": 5},
        "json": {"data_source": json_data_path, "flatten_depth": 2}
    }
    parse_results = parser.run_multimodal_parse(parse_config)

    # 4. å¤šæ¨¡æ€æ•°æ®èåˆ
    if len(parse_results) >= 2:
        fused_data = parser.fuse_multimodal_data(parse_results, align_method="sample_count")
        print(f"\nğŸ“Š å¤šæ¨¡æ€èåˆåæ•°æ®ç»´åº¦ï¼š{len(fused_data)}è¡Œ Ã— {len(fused_data.columns)}åˆ—")

    # 5. æŸ¥çœ‹è§£æå†å²
    print("\nğŸ“œ è§£æå†å²æ±‡æ€»ï¼š")
    for idx, history in enumerate(parser.parse_history):
        print(f"  {idx+1}. æ¨¡æ€ï¼š{history['modality']} | æ ·æœ¬æ•°ï¼š{history['sample_count']} | åˆ—æ•°ï¼š{history['col_count']}")

    print("\nğŸ‰ å¤šæ¨¡æ€è§£ææ¨¡å—æµ‹è¯•å®Œæˆï¼æ‰€æœ‰è§£æåæ•°æ®å·²ä¿å­˜è‡³ ./multimodal_processed")