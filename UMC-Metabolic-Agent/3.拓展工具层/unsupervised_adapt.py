# -*- coding: utf-8 -*-
"""
UMC-Metabolic-Agent æ— ç›‘ç£è‡ªé€‚åº”æ¨¡å—ï¼ˆé¢†åŸŸè‡ªä¸»è¯†åˆ«+å‚æ•°è‡ªåŠ¨è°ƒä¼˜+é›¶æ ‡æ³¨é€‚é…ï¼‰
æ ¸å¿ƒé€»è¾‘ï¼šæ— éœ€äººå·¥æ ‡æ³¨/å‚æ•°è°ƒæ•´ï¼Œè‡ªä¸»è¯†åˆ«æ•°æ®é¢†åŸŸã€æå–ç‰¹å¾ã€é€‚é…ä»£è°¢å¾ªç¯/ç­–ç•¥æƒé‡
è®¾è®¡åŸåˆ™ï¼šæ— ç›‘ç£ã€è‡ªé©±åŠ¨ã€é¢†åŸŸæ— å…³ã€æ•ˆæœå¯è¯„ä¼°ï¼Œé€‚é…æ–°æ‰‹é›¶é…ç½®ä½¿ç”¨å¤šé¢†åŸŸæ•°æ®
"""
import pandas as pd
import numpy as np
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import pairwise_distances
import json
import os
import time
from typing import Dict, Any, List, Optional, Tuple
import warnings
warnings.filterwarnings("ignore")

# å¯¼å…¥æ ¸å¿ƒå·¥å…·
from tool_build import UMCAgent
from tool_config import ConfigManager

class UnsupervisedAdaptor:
    """æ— ç›‘ç£è‡ªé€‚åº”å™¨ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼šé¢†åŸŸè¯†åˆ«ã€ç‰¹å¾æå–ã€å‚æ•°è‡ªé€‚åº”ã€æ•ˆæœè¯„ä¼°ï¼‰"""
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        """
        åˆå§‹åŒ–æ— ç›‘ç£è‡ªé€‚åº”å™¨
        :param config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹ï¼ˆé»˜è®¤è‡ªåŠ¨åˆå§‹åŒ–ï¼‰
        """
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = config_manager if config_manager else ConfigManager()
        # åˆå§‹åŒ–é¢†åŸŸç‰¹å¾åº“ï¼ˆå†…ç½®é‡å­/åŸå­/ç‰©æµé¢†åŸŸçš„åŸºå‡†ç‰¹å¾ï¼Œç”¨äºåŒ¹é…ï¼‰
        self.domain_feature_lib = self._init_domain_feature_lib()
        # åˆå§‹åŒ–è‡ªé€‚åº”çŠ¶æ€
        self.adapt_history = []
        self.current_domain = "unknown"  # å½“å‰è¯†åˆ«çš„é¢†åŸŸ
        self.current_adapt_params = {}   # å½“å‰è‡ªé€‚åº”è°ƒæ•´çš„å‚æ•°
        self.current_feature = None      # å½“å‰æ•°æ®çš„æ ¸å¿ƒç‰¹å¾

    def _init_domain_feature_lib(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–å†…ç½®é¢†åŸŸç‰¹å¾åº“ï¼ˆåŸºå‡†ç‰¹å¾ï¼Œç”¨äºæ— ç›‘ç£åŒ¹é…ï¼‰"""
        # æ¯ä¸ªé¢†åŸŸçš„åŸºå‡†ç‰¹å¾ï¼šå‡å€¼ã€æ–¹å·®ã€ç‰¹å¾ç›¸å…³æ€§ã€ä¸»æˆåˆ†å æ¯”ï¼ˆåŸºäºå¤§é‡æ ·æœ¬ç»Ÿè®¡ï¼‰
        domain_feature_lib = {
            "quantum": {
                "desc": "é‡å­é¢†åŸŸæ•°æ®ï¼ˆqubitç¨³å®šæ€§ã€èƒ½è€—ã€ç‰©è´¨è¾“å‡ºï¼‰",
                "feature_cols": ["qubit_stability", "energy_consumption", "matter_output"],
                "mean": [0.45, 0.4, 0.35],          # å„ç‰¹å¾å‡å€¼
                "std": [0.2, 0.18, 0.15],           # å„ç‰¹å¾æ–¹å·®
                "corr_matrix": [[1.0, -0.7, 0.6],   # ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ
                                [-0.7, 1.0, -0.5],
                                [0.6, -0.5, 1.0]],
                "pca_var_ratio": [0.75, 0.15, 0.1]  # ä¸»æˆåˆ†æ–¹å·®å æ¯”
            },
            "atomic": {
                "desc": "åŸå­é¢†åŸŸæ•°æ®ï¼ˆåŸå­é¢‘ç‡ã€èƒ½æ•ˆã€ç²’å­äº§ç‡ï¼‰",
                "feature_cols": ["atomic_frequency", "energy_efficiency", "particle_yield"],
                "mean": [0.5, 0.42, 0.38],
                "std": [0.22, 0.19, 0.17],
                "corr_matrix": [[1.0, -0.65, 0.55],
                                [-0.65, 1.0, -0.45],
                                [0.55, -0.45, 1.0]],
                "pca_var_ratio": [0.7, 0.2, 0.1]
            },
            "logistics": {
                "desc": "ç‰©æµé¢†åŸŸæ•°æ®ï¼ˆç‰©æµæ•ˆç‡ã€è¿è¾“æˆæœ¬ã€é…é€é€Ÿåº¦ï¼‰",
                "feature_cols": ["logistics_efficiency", "transport_cost", "delivery_speed"],
                "mean": [0.48, 0.45, 0.4],
                "std": [0.18, 0.2, 0.16],
                "corr_matrix": [[1.0, -0.8, 0.7],
                                [-0.8, 1.0, -0.6],
                                [0.7, -0.6, 1.0]],
                "pca_var_ratio": [0.8, 0.12, 0.08]
            }
        }
        # ä¿å­˜é¢†åŸŸç‰¹å¾åº“åˆ°æœ¬åœ°ï¼ˆä¾¿äºæ‰©å±•ï¼‰
        lib_path = "./domain_feature_lib.json"
        if not os.path.exists(lib_path):
            with open(lib_path, "w", encoding="utf-8") as f:
                json.dump(domain_feature_lib, f, ensure_ascii=False, indent=2)
        return domain_feature_lib

    def extract_unsupervised_feature(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ— ç›‘ç£ç‰¹å¾æå–ï¼ˆæ ¸å¿ƒï¼šæå–æ•°æ®çš„æ ¸å¿ƒç»Ÿè®¡ç‰¹å¾ï¼Œç”¨äºé¢†åŸŸåŒ¹é…ï¼‰
        :param data: åŸå§‹/æ ‡å‡†åŒ–æ•°æ®ï¼ˆæ”¯æŒä»»æ„æ•°å€¼å‹æ•°æ®ï¼‰
        :return: æ•°æ®çš„æ ¸å¿ƒæ— ç›‘ç£ç‰¹å¾
        """
        print("\nğŸ” å¼€å§‹æ— ç›‘ç£ç‰¹å¾æå–...")
        # 1. æ•°æ®é¢„å¤„ç†ï¼ˆå»é‡ã€å¡«å……ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–ï¼‰
        data_clean = data.copy().drop_duplicates()
        data_clean = data_clean.fillna(data_clean.mean())
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_clean)
        data_scaled_df = pd.DataFrame(data_scaled, columns=data_clean.columns)

        # 2. æå–åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        feature = {
            "feature_cols": data_clean.columns.tolist(),
            "sample_count": len(data_clean),
            "mean": data_scaled.mean(axis=0).tolist(),
            "std": data_scaled.std(axis=0).tolist(),
            "corr_matrix": data_scaled_df.corr().values.tolist(),  # ç›¸å…³æ€§çŸ©é˜µ
            "pca_var_ratio": []  # ä¸»æˆåˆ†æ–¹å·®å æ¯”
        }

        # 3. æå–PCAç‰¹å¾ï¼ˆä¸»æˆåˆ†åˆ†æï¼‰
        pca_n = min(3, len(data_clean.columns))  # æœ€å¤šå–å‰3ä¸ªä¸»æˆåˆ†
        pca = PCA(n_components=pca_n)
        pca.fit(data_scaled)
        feature["pca_var_ratio"] = pca.explained_variance_ratio_.tolist()

        # 4. è¡¥å……åˆ†å¸ƒç‰¹å¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰
        feature["normality_pvalue"] = [stats.shapiro(data_scaled[:, i])[1] for i in range(min(3, data_scaled.shape[1]))]

        # ä¿å­˜å½“å‰ç‰¹å¾
        self.current_feature = feature
        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼šæ ·æœ¬æ•°{feature['sample_count']} | ç‰¹å¾åˆ—{feature['feature_cols']}")
        return feature

    def match_domain(self, feature: Dict[str, Any]) -> Tuple[str, float]:
        """
        æ— ç›‘ç£é¢†åŸŸåŒ¹é…ï¼ˆæ ¸å¿ƒï¼šå¯¹æ¯”ç‰¹å¾åº“ï¼Œè¯†åˆ«æ•°æ®æ‰€å±é¢†åŸŸï¼‰
        :param feature: æ•°æ®çš„æ— ç›‘ç£ç‰¹å¾
        :return: (åŒ¹é…çš„é¢†åŸŸåç§°, åŒ¹é…ç›¸ä¼¼åº¦[0~1])
        """
        print("\nğŸ¯ å¼€å§‹æ— ç›‘ç£é¢†åŸŸåŒ¹é…...")
        similarity_scores = {}

        # éå†é¢†åŸŸç‰¹å¾åº“ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
        for domain, domain_feature in self.domain_feature_lib.items():
            # 1. å‡å€¼ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            mean_sim = 1 - pairwise_distances([feature["mean"]], [domain_feature["mean"]], metric="cosine")[0][0]
            # 2. æ–¹å·®ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            std_sim = 1 - pairwise_distances([feature["std"]], [domain_feature["std"]], metric="cosine")[0][0]
            # 3. ç›¸å…³æ€§çŸ©é˜µç›¸ä¼¼åº¦ï¼ˆå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            corr_sim = 1 - pairwise_distances(feature["corr_matrix"], domain_feature["corr_matrix"], metric="cosine").mean()
            # 4. PCAæ–¹å·®å æ¯”ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            # å¯¹é½PCAç»´åº¦ï¼ˆä¸è¶³è¡¥0ï¼‰
            pca_self = feature["pca_var_ratio"] + [0]*(3-len(feature["pca_var_ratio"]))
            pca_domain = domain_feature["pca_var_ratio"] + [0]*(3-len(domain_feature["pca_var_ratio"]))
            pca_sim = 1 - pairwise_distances([pca_self], [pca_domain], metric="cosine")[0][0]

            # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
            total_sim = (mean_sim * 0.3) + (std_sim * 0.2) + (corr_sim * 0.3) + (pca_sim * 0.2)
            similarity_scores[domain] = max(0, min(1, total_sim))  # é™åˆ¶åœ¨0~1ä¹‹é—´

        # ç¡®å®šåŒ¹é…é¢†åŸŸï¼ˆç›¸ä¼¼åº¦æœ€é«˜ä¸”â‰¥é˜ˆå€¼0.5ï¼Œå¦åˆ™ä¸ºunknownï¼‰
        max_sim_domain = max(similarity_scores.items(), key=lambda x: x[1])
        match_domain = max_sim_domain[0] if max_sim_domain[1] >= 0.5 else "unknown"
        match_sim = max_sim_domain[1] if match_domain != "unknown" else 0.0

        # æ‰“å°åŒ¹é…ç»“æœ
        print("é¢†åŸŸåŒ¹é…å¾—åˆ†ï¼š")
        for domain, score in similarity_scores.items():
            print(f"  - {domain}ï¼š{score:.3f}")
        print(f"âœ… åŒ¹é…ç»“æœï¼š{match_domain}ï¼ˆç›¸ä¼¼åº¦ï¼š{match_sim:.3f}ï¼‰")

        # æ›´æ–°å½“å‰é¢†åŸŸ
        self.current_domain = match_domain
        return match_domain, match_sim

    def adapt_params(self, domain: str, feature: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ— ç›‘ç£å‚æ•°è‡ªé€‚åº”è°ƒæ•´ï¼ˆæ ¸å¿ƒï¼šæ ¹æ®é¢†åŸŸè‡ªåŠ¨è°ƒæ•´ä»£è°¢/ç­–ç•¥å‚æ•°ï¼‰
        :param domain: åŒ¹é…çš„é¢†åŸŸ
        :param feature: æ•°æ®çš„æ— ç›‘ç£ç‰¹å¾
        :return: è‡ªé€‚åº”è°ƒæ•´åçš„å‚æ•°
        """
        print(f"\nâš™ï¸  å¼€å§‹{domain}é¢†åŸŸå‚æ•°è‡ªé€‚åº”è°ƒæ•´...")
        # å¤‡ä»½å½“å‰é…ç½®ï¼ˆè°ƒæ•´å‰ï¼‰
        self.config_manager.backup_config(backup_name=f"pre_adapt_{domain}")

        # åˆå§‹åŒ–è‡ªé€‚åº”å‚æ•°ï¼ˆåŸºäºé¢†åŸŸå’Œç‰¹å¾ï¼‰
        adapt_params = {
            "domain": domain,
            "adapt_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "metabolism_params": {},
            "strategy_params": {},
            "agi_l3_params": {}
        }

        # 1. é¢†åŸŸä¸“å±å‚æ•°è°ƒæ•´
        if domain == "quantum":
            # é‡å­é¢†åŸŸï¼šæé«˜ç¨³å®šæ€§æƒé‡ï¼Œé™ä½å¾ªç¯é€Ÿåº¦
            adapt_params["metabolism_params"] = {
                "core_factor_weight": min(1.0, float(self.config_manager.param_cfg["METABOLISM"]["core_factor_weight"]) * 1.1),
                "stability_threshold": min(1.0, float(self.config_manager.param_cfg["METABOLISM"]["stability_threshold"]) * 1.05),
                "cycle_speed": max(0.01, float(self.config_manager.param_cfg["BASIC"]["cycle_speed"]) * 0.9)
            }
            adapt_params["strategy_params"] = {
                "qubit_stability": 0.9,
                "atomic_frequency": 0.5,
                "logistics_efficiency": 0.5,
                "unknown_domain": 0.5
            }
            adapt_params["agi_l3_params"] = {
                "goal_discovery_threshold": max(0.1, float(self.config_manager.param_cfg["AGI_L3"]["goal_discovery_threshold"]) * 0.9)
            }

        elif domain == "atomic":
            # åŸå­é¢†åŸŸï¼šæé«˜èƒ½è€—ä¸Šé™ï¼Œè°ƒæ•´ç›®æ ‡å‘ç°é˜ˆå€¼
            adapt_params["metabolism_params"] = {
                "energy_consumption_limit": min(1.0, float(self.config_manager.param_cfg["METABOLISM"]["energy_consumption_limit"]) * 1.05),
                "core_factor_weight": min(1.0, float(self.config_manager.param_cfg["METABOLISM"]["core_factor_weight"]) * 1.05)
            }
            adapt_params["strategy_params"] = {
                "qubit_stability": 0.5,
                "atomic_frequency": 0.9,
                "logistics_efficiency": 0.5,
                "unknown_domain": 0.5
            }
            adapt_params["agi_l3_params"] = {
                "self_learning_feedback_rate": min(1.0, float(self.config_manager.param_cfg["AGI_L3"]["self_learning_feedback_rate"]) * 1.1)
            }

        elif domain == "logistics":
            # ç‰©æµé¢†åŸŸï¼šæé«˜å¾ªç¯é€Ÿåº¦ï¼Œé™ä½ç¨³å®šæ€§é˜ˆå€¼
            adapt_params["metabolism_params"] = {
                "cycle_speed": min(1.0, float(self.config_manager.param_cfg["BASIC"]["cycle_speed"]) * 1.1),
                "stability_threshold": max(0.5, float(self.config_manager.param_cfg["METABOLISM"]["stability_threshold"]) * 0.95)
            }
            adapt_params["strategy_params"] = {
                "qubit_stability": 0.5,
                "atomic_frequency": 0.5,
                "logistics_efficiency": 0.9,
                "unknown_domain": 0.5
            }
            adapt_params["agi_l3_params"] = {
                "auto_recovery_fault_threshold": min(10, int(self.config_manager.param_cfg["AGI_L3"]["auto_recovery_fault_threshold"]) + 1)
            }

        else:
            # æœªçŸ¥é¢†åŸŸï¼šä¿å®ˆè°ƒæ•´ï¼Œæé«˜å®¹é”™æ€§
            adapt_params["metabolism_params"] = {
                "core_factor_weight": 0.7,
                "stability_threshold": 0.75,
                "cycle_speed": 0.05
            }
            adapt_params["strategy_params"] = {
                "unknown_domain": 0.8
            }
            adapt_params["agi_l3_params"] = {
                "auto_recovery_fault_threshold": 2,
                "goal_discovery_threshold": 0.4
            }

        # 2. åŸºäºæ•°æ®ç‰¹å¾çš„åŠ¨æ€è°ƒæ•´ï¼ˆè¡¥å……é¢†åŸŸé€šç”¨è°ƒæ•´ï¼‰
        # æ ¹æ®æ ·æœ¬é‡è°ƒæ•´ç¼“å­˜å¤§å°
        sample_count = feature["sample_count"]
        adapt_params["metabolism_params"]["data_cache_size"] = min(1000, max(10, int(sample_count * 0.1)))

        # 3. åº”ç”¨å‚æ•°è°ƒæ•´åˆ°é…ç½®æ–‡ä»¶
        # æ›´æ–°ä»£è°¢/åŸºç¡€å‚æ•°
        for param, value in adapt_params["metabolism_params"].items():
            if param in self.config_manager.param_cfg["METABOLISM"]:
                self.config_manager.param_cfg["METABOLISM"][param] = str(value)
            elif param in self.config_manager.param_cfg["BASIC"]:
                self.config_manager.param_cfg["BASIC"][param] = str(value)
        # æ›´æ–°ç­–ç•¥å‚æ•°
        for param, value in adapt_params["strategy_params"].items():
            if param in self.config_manager.param_cfg["STRATEGY"]:
                self.config_manager.param_cfg["STRATEGY"][param] = str(value)
        # æ›´æ–°AGI_L3å‚æ•°
        for param, value in adapt_params["agi_l3_params"].items():
            if param in self.config_manager.param_cfg["AGI_L3"]:
                self.config_manager.param_cfg["AGI_L3"][param] = str(value)

        # ä¿å­˜é…ç½®
        self.config_manager._save_param_config()
        print("âœ… å‚æ•°è‡ªé€‚åº”è°ƒæ•´å®Œæˆï¼Œè°ƒæ•´é¡¹ï¼š")
        for param_type, params in adapt_params.items():
            if param_type in ["metabolism_params", "strategy_params", "agi_l3_params"]:
                for k, v in params.items():
                    print(f"  - {param_type}.{k}ï¼š{v}")

        # æ›´æ–°å½“å‰è‡ªé€‚åº”å‚æ•°
        self.current_adapt_params = adapt_params
        return adapt_params

    def evaluate_adapt_effect(self, umc_agent: UMCAgent, data: pd.DataFrame) -> Dict[str, float]:
        """
        æ— ç›‘ç£è‡ªé€‚åº”æ•ˆæœè¯„ä¼°ï¼ˆæ ¸å¿ƒï¼šè¯„ä¼°è°ƒæ•´åæ™ºèƒ½ä½“çš„è¿è¡Œæ•ˆæœï¼‰
        :param umc_agent: è‡ªé€‚åº”åçš„UMCAgentå®ä¾‹
        :param data: æµ‹è¯•æ•°æ®
        :return: æ•ˆæœè¯„ä¼°æŒ‡æ ‡ï¼ˆç¨³å®šæ€§ã€ä¸€è‡´æ€§ã€æ•ˆç‡ï¼‰
        """
        print("\nğŸ“Š å¼€å§‹æ— ç›‘ç£è‡ªé€‚åº”æ•ˆæœè¯„ä¼°...")
        # è¿è¡Œæ™ºèƒ½ä½“è·å–ç»“æœ
        run_result = umc_agent.run(data, domain_name=self.current_domain)

        # æå–è¯„ä¼°æŒ‡æ ‡ï¼ˆæ— ç›‘ç£ï¼Œæ— éœ€æ ‡æ³¨ï¼‰
        metrics = {
            # 1. ä»£è°¢ç¨³å®šæ€§ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼šè¶Šé«˜è¶Šå¥½ï¼‰
            "metabolic_stability": run_result["metabolic_result"]["stability_score"],
            # 2. ç»“æœä¸€è‡´æ€§ï¼ˆå¤šæ¬¡è¿è¡Œçš„ç¨³å®šæ€§å¾—åˆ†æ–¹å·®ï¼šè¶Šä½è¶Šå¥½ï¼Œè½¬æ¢ä¸º0~1ï¼‰
            "result_consistency": self._calculate_consistency(umc_agent, data),
            # 3. è¿è¡Œæ•ˆç‡ï¼ˆå¾ªç¯æ¬¡æ•°/æ ·æœ¬æ•°ï¼šè¶Šä½è¶Šå¥½ï¼Œè½¬æ¢ä¸º0~1ï¼‰
            "run_efficiency": max(0, 1 - (run_result["metabolic_result"]["cycle_count"] / len(data))),
            # 4. æ€§èƒ½è¾¾æ ‡ç‡ï¼ˆç›¸å¯¹é˜ˆå€¼ï¼šè¶Šé«˜è¶Šå¥½ï¼‰
            "performance_rate": run_result["perf_score"] / float(self.config_manager.param_cfg["VALIDATION"]["blackbox_test_threshold"])
        }
        # å½’ä¸€åŒ–æ‰€æœ‰æŒ‡æ ‡åˆ°0~1
        metrics = {k: max(0, min(1, v)) for k, v in metrics.items()}
        # ç»¼åˆæ•ˆæœå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        metrics["comprehensive_score"] = (
            metrics["metabolic_stability"] * 0.4 +
            metrics["result_consistency"] * 0.2 +
            metrics["run_efficiency"] * 0.2 +
            metrics["performance_rate"] * 0.2
        )

        # æ‰“å°è¯„ä¼°ç»“æœ
        print("âœ… è‡ªé€‚åº”æ•ˆæœè¯„ä¼°ç»“æœï¼š")
        for metric, score in metrics.items():
            print(f"  - {metric}ï¼š{score:.3f}")
        print(f"  - ç»¼åˆæ•ˆæœå¾—åˆ†ï¼š{metrics['comprehensive_score']:.3f}ï¼ˆâ‰¥0.7ä¸ºä¼˜ç§€ï¼‰")

        return metrics

    def _calculate_consistency(self, umc_agent: UMCAgent, data: pd.DataFrame, run_times: int = 3) -> float:
        """è®¡ç®—å¤šæ¬¡è¿è¡Œçš„ç»“æœä¸€è‡´æ€§ï¼ˆæ— ç›‘ç£æŒ‡æ ‡ï¼‰"""
        stability_scores = []
        for i in range(run_times):
            result = umc_agent.run(data, domain_name=self.current_domain)
            stability_scores.append(result["metabolic_result"]["stability_score"])
        # è®¡ç®—æ–¹å·®ï¼Œè½¬æ¢ä¸ºä¸€è‡´æ€§å¾—åˆ†ï¼ˆæ–¹å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜ï¼‰
        var = np.var(stability_scores)
        consistency = max(0, 1 - min(var * 10, 1))  # æ–¹å·®*10åé™åˆ¶åœ¨0~1ï¼Œå–å
        return consistency

    def run_full_adapt(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        ä¸€é”®è¿è¡Œå…¨æµç¨‹æ— ç›‘ç£è‡ªé€‚åº”ï¼ˆç‰¹å¾æå–â†’é¢†åŸŸåŒ¹é…â†’å‚æ•°è°ƒæ•´â†’æ•ˆæœè¯„ä¼°ï¼‰
        :param data: ä»»æ„é¢†åŸŸçš„æ•°å€¼å‹æ•°æ®
        :return: è‡ªé€‚åº”å…¨æµç¨‹ç»“æœ
        """
        print("ğŸš€ å¼€å§‹UMCæ™ºèƒ½ä½“æ— ç›‘ç£è‡ªé€‚åº”å…¨æµç¨‹...")
        start_time = time.time()

        # 1. æ— ç›‘ç£ç‰¹å¾æå–
        feature = self.extract_unsupervised_feature(data)

        # 2. æ— ç›‘ç£é¢†åŸŸåŒ¹é…
        domain, similarity = self.match_domain(feature)

        # 3. å‚æ•°è‡ªé€‚åº”è°ƒæ•´
        adapt_params = self.adapt_params(domain, feature)

        # 4. åˆå§‹åŒ–è‡ªé€‚åº”åçš„æ™ºèƒ½ä½“
        umc_agent_adapted = UMCAgent()  # è‡ªåŠ¨åŠ è½½è°ƒæ•´åçš„é…ç½®

        # 5. è‡ªé€‚åº”æ•ˆæœè¯„ä¼°
        adapt_effect = self.evaluate_adapt_effect(umc_agent_adapted, data)

        # 6. æ±‡æ€»è‡ªé€‚åº”ç»“æœ
        full_result = {
            "start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_duration": f"{time.time() - start_time:.2f}s",
            "data_info": {"sample_count": len(data), "feature_cols": data.columns.tolist()},
            "domain_match": {"domain": domain, "similarity": similarity},
            "adapt_params": adapt_params,
            "adapt_effect": adapt_effect,
            "is_adapt_successful": adapt_effect["comprehensive_score"] >= 0.6  # â‰¥0.6ä¸ºæˆåŠŸ
        }

        # è®°å½•è‡ªé€‚åº”å†å²
        self.adapt_history.append(full_result)

        # ä¿å­˜è‡ªé€‚åº”ç»“æœåˆ°æœ¬åœ°
        result_dir = "./adapt_results"
        os.makedirs(result_dir, exist_ok=True)
        result_path = os.path.join(result_dir, f"adapt_result_{domain}_{time.strftime('%Y%m%d%H%M%S')}.json")
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(full_result, f, ensure_ascii=False, indent=2)

        # æ‰“å°æœ€ç»ˆç»“æœ
        print("\nğŸ æ— ç›‘ç£è‡ªé€‚åº”å…¨æµç¨‹å®Œæˆï¼")
        print(f"  - æ•°æ®è§„æ¨¡ï¼š{len(data)}è¡Œ Ã— {len(data.columns)}åˆ—")
        print(f"  - è¯†åˆ«é¢†åŸŸï¼š{domain}ï¼ˆç›¸ä¼¼åº¦{similarity:.3f}ï¼‰")
        print(f"  - ç»¼åˆæ•ˆæœï¼š{adapt_effect['comprehensive_score']:.3f}")
        print(f"  - è‡ªé€‚åº”æˆåŠŸï¼š{full_result['is_adapt_successful']}")
        print(f"  - ç»“æœä¿å­˜ï¼š{result_path}")

        return full_result

    def add_custom_domain(self, domain_name: str, domain_data: pd.DataFrame, domain_desc: str = "") -> None:
        """
        æ‰©å±•è‡ªå®šä¹‰é¢†åŸŸåˆ°ç‰¹å¾åº“ï¼ˆæ— ç›‘ç£ï¼Œæ— éœ€æ ‡æ³¨ï¼‰
        :param domain_name: è‡ªå®šä¹‰é¢†åŸŸåç§°
        :param domain_data: è‡ªå®šä¹‰é¢†åŸŸçš„æ ·æœ¬æ•°æ®
        :param domain_desc: é¢†åŸŸæè¿°
        """
        print(f"\nğŸ†• å¼€å§‹æ‰©å±•è‡ªå®šä¹‰é¢†åŸŸï¼š{domain_name}...")
        # æå–è‡ªå®šä¹‰é¢†åŸŸçš„ç‰¹å¾
        domain_feature = self.extract_unsupervised_feature(domain_data)
        # æ„å»ºè‡ªå®šä¹‰é¢†åŸŸç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼Œä¿ç•™æ ¸å¿ƒï¼‰
        custom_domain_feature = {
            "desc": domain_desc if domain_desc else f"è‡ªå®šä¹‰é¢†åŸŸï¼š{domain_name}",
            "feature_cols": domain_feature["feature_cols"],
            "mean": domain_feature["mean"],
            "std": domain_feature["std"],
            "corr_matrix": domain_feature["corr_matrix"],
            "pca_var_ratio": domain_feature["pca_var_ratio"]
        }
        # æ·»åŠ åˆ°é¢†åŸŸç‰¹å¾åº“
        self.domain_feature_lib[domain_name] = custom_domain_feature
        # ä¿å­˜æ›´æ–°åçš„ç‰¹å¾åº“
        with open("./domain_feature_lib.json", "w", encoding="utf-8") as f:
            json.dump(self.domain_feature_lib, f, ensure_ascii=False, indent=2)
        print(f"âœ… è‡ªå®šä¹‰é¢†åŸŸ{domain_name}å·²æ·»åŠ åˆ°ç‰¹å¾åº“ï¼Œæ”¯æŒè‡ªåŠ¨åŒ¹é…ï¼")

# æ— ç›‘ç£è‡ªé€‚åº”æ¨¡å—éªŒè¯å…¥å£ï¼ˆä¸€ç«™å¼æµ‹è¯•ï¼‰
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–æ— ç›‘ç£è‡ªé€‚åº”å™¨
    adaptor = UnsupervisedAdaptor()
    print("ğŸš€ æ— ç›‘ç£è‡ªé€‚åº”å™¨åˆå§‹åŒ–å®Œæˆï¼")

    # 2. ç”Ÿæˆä¸åŒé¢†åŸŸçš„æµ‹è¯•æ•°æ®ï¼ˆé‡å­/åŸå­/ç‰©æµ/è‡ªå®šä¹‰ï¼‰
    from tool_build import create_test_data
    # æµ‹è¯•1ï¼šé‡å­é¢†åŸŸæ•°æ®
    print("\n=== æµ‹è¯•1ï¼šé‡å­é¢†åŸŸæ•°æ®è‡ªé€‚åº” ===")
    quantum_data = create_test_data(domain_name="quantum", sample_count=200)
    quantum_adapt_result = adaptor.run_full_adapt(quantum_data)

    # æµ‹è¯•2ï¼šè‡ªå®šä¹‰é¢†åŸŸæ•°æ®ï¼ˆæ‰©å±•+è‡ªé€‚åº”ï¼‰
    print("\n=== æµ‹è¯•2ï¼šè‡ªå®šä¹‰é¢†åŸŸæ•°æ®è‡ªé€‚åº” ===")
    # ç”Ÿæˆè‡ªå®šä¹‰æ•°æ®ï¼ˆæ¯”å¦‚é‡‘èé¢†åŸŸï¼‰
    custom_data = pd.DataFrame({
        "risk_score": np.random.rand(150) * 0.9,
        "return_rate": np.random.rand(150) * 0.8,
        "liquidity": np.random.rand(150) * 0.7
    })
    # æ‰©å±•è‡ªå®šä¹‰é¢†åŸŸ
    adaptor.add_custom_domain("finance", custom_data, "é‡‘èé¢†åŸŸæ•°æ®ï¼ˆé£é™©è¯„åˆ†ã€æ”¶ç›Šç‡ã€æµåŠ¨æ€§ï¼‰")
    # è‡ªé€‚åº”è‡ªå®šä¹‰é¢†åŸŸæ•°æ®
    custom_adapt_result = adaptor.run_full_adapt(custom_data)

    # 3. æŸ¥çœ‹è‡ªé€‚åº”å†å²
    print("\n=== è‡ªé€‚åº”å†å²æ±‡æ€» ===")
    print(f"è‡ªé€‚åº”æ¬¡æ•°ï¼š{len(adaptor.adapt_history)}")
    for idx, history in enumerate(adaptor.adapt_history):
        print(f"  {idx+1}. é¢†åŸŸï¼š{history['domain_match']['domain']} | ç»¼åˆå¾—åˆ†ï¼š{history['adapt_effect']['comprehensive_score']:.3f}")

    print("\nğŸ‰ æ— ç›‘ç£è‡ªé€‚åº”æ¨¡å—æµ‹è¯•å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ ./adapt_results")