# -*- coding: utf-8 -*-
"""
UMC-Metabolic-Agent é€šç”¨å‘½ä»¤è¡Œæ¨¡å—ï¼ˆç»Ÿä¸€CLIå…¥å£+å…¨æµç¨‹è‡ªåŠ¨åŒ–ï¼‰
æ ¸å¿ƒé€»è¾‘ï¼šæä¾›æ ‡å‡†åŒ–å‘½ä»¤è¡Œæ¥å£ï¼Œæ•´åˆæ™ºèƒ½ä½“å…¨ç”Ÿå‘½å‘¨æœŸæ“ä½œï¼Œæ”¯æŒå•å‘½ä»¤/ä¸€ç«™å¼æ‰§è¡Œ
è®¾è®¡åŸåˆ™ï¼šæ–°æ‰‹å‹å¥½ã€å‚æ•°ç®€åŒ–ã€åŠŸèƒ½å…¨è¦†ç›–ã€è¾“å‡ºå¯è§†åŒ–ï¼Œé€‚é…é›¶é…ç½®å¿«é€Ÿä½¿ç”¨
"""
import argparse
import sys
import os
import json
import time
import warnings
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Union

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from result_analysis import ResultAnalyzer
    from report_generator import ReportGenerator
except ImportError as e:
    print(f"âš ï¸  æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥ï¼š{e}")
    print("âš ï¸  è¯·ç¡®ä¿result_analysis.pyå’Œreport_generator.pyåœ¨å½“å‰ç›®å½•")
    ResultAnalyzer = None
    ReportGenerator = None

warnings.filterwarnings("ignore")

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå‘½ä»¤è¡Œè¾“å‡ºï¼‰
sys.stdout.reconfigure(encoding='utf-8')

class UniversalCmd:
    """é€šç”¨å‘½ä»¤è¡Œæ§åˆ¶å™¨ï¼ˆæ ¸å¿ƒï¼šè§£æå‘½ä»¤ã€è°ƒåº¦æ¨¡å—ã€æ‰§è¡Œå…¨æµç¨‹æ“ä½œï¼‰"""
    def __init__(self):
        """åˆå§‹åŒ–å‘½ä»¤è¡Œæ§åˆ¶å™¨"""
        # åŸºç¡€é…ç½®
        self.base_dir = os.getcwd()
        self.output_root = "./umc_agent_output"
        os.makedirs(self.output_root, exist_ok=True)
        
        # åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—
        self.analyzer = ResultAnalyzer(output_dir=f"{self.output_root}/analysis") if ResultAnalyzer else None
        self.report_generator = ReportGenerator(output_dir=f"{self.output_root}/reports") if ReportGenerator else None
        
        # å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
        self.parser = self._create_arg_parser()
        
        # æ“ä½œå†å²
        self.operation_history = []

    def _create_arg_parser(self) -> argparse.ArgumentParser:
        """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ï¼ˆæ ¸å¿ƒï¼šå®šä¹‰æ‰€æœ‰æ”¯æŒçš„å‘½ä»¤å’Œå‚æ•°ï¼‰"""
        parser = argparse.ArgumentParser(
            prog="UMC-Metabolic-Agent",
            description="UMCæ™ºèƒ½ä½“é€šç”¨å‘½ä»¤è¡Œå·¥å…· - æ•´åˆè¿è¡Œ/åˆ†æ/æŠ¥å‘Š/è‡ªé€‚åº”/å¤šæ¨¡æ€å…¨æµç¨‹",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  1. ä¸€é”®æ‰§è¡Œå…¨æµç¨‹ï¼š
     python universal_cmd.py all --data-path ./test_data.csv --domain quantum
     
  2. ä»…æ‰§è¡Œç»“æœåˆ†æï¼š
     python universal_cmd.py analyze --data-path ./run_result.csv --target-col matter_output
     
  3. ä»…ç”ŸæˆæŠ¥å‘Šï¼š
     python universal_cmd.py report --analysis-path ./analysis/result.json --format md html
     
  4. æŸ¥çœ‹å¸®åŠ©ï¼š
     python universal_cmd.py -h
     python universal_cmd.py analyze -h
            """
        )
        
        # å­å‘½ä»¤è§£æå™¨
        subparsers = parser.add_subparsers(dest="command", required=True, help="æ“ä½œå‘½ä»¤")
        
        # 1. allå‘½ä»¤ï¼šä¸€ç«™å¼æ‰§è¡Œæ‰€æœ‰æ“ä½œï¼ˆæ–°æ‰‹æ¨èï¼‰
        parser_all = subparsers.add_parser(
            "all", 
            help="ä¸€ç«™å¼æ‰§è¡Œï¼šè¿è¡Œæ™ºèƒ½ä½“â†’ç»“æœåˆ†æâ†’ç”ŸæˆæŠ¥å‘Šï¼ˆæ–°æ‰‹æ¨èï¼‰",
            description="ä¸€ç«™å¼æ‰§è¡Œæ™ºèƒ½ä½“å…¨æµç¨‹æ“ä½œï¼Œè‡ªåŠ¨å®Œæˆè¿è¡Œã€åˆ†æã€æŠ¥å‘Šç”Ÿæˆ"
        )
        parser_all.add_argument("--data-path", "-d", type=str, required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSV/Excelï¼‰")
        parser_all.add_argument("--domain", "-dm", type=str, default="general", help="ç›®æ ‡é¢†åŸŸï¼ˆå¦‚quantum/biology/chemistryï¼‰")
        parser_all.add_argument("--target-col", "-t", type=str, default="matter_output", help="ç›®æ ‡åˆ†æåˆ—å")
        parser_all.add_argument("--report-formats", "-f", nargs="+", default=["md", "html"], choices=["md", "html", "pdf"], help="æŠ¥å‘Šè¾“å‡ºæ ¼å¼")
        parser_all.add_argument("--with-plots", action="store_true", default=True, help="æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        parser_all.add_argument("--output-name", "-o", type=str, default=f"umc_agent_{time.strftime('%Y%m%d')}", help="è¾“å‡ºæ–‡ä»¶å‰ç¼€å")
        
        # 2. runå‘½ä»¤ï¼šä»…è¿è¡Œæ™ºèƒ½ä½“
        parser_run = subparsers.add_parser(
            "run", 
            help="ä»…è¿è¡ŒUMCæ™ºèƒ½ä½“ï¼ˆç”Ÿæˆè¿è¡Œç»“æœæ•°æ®ï¼‰",
            description="è¿è¡ŒUMCæ™ºèƒ½ä½“ï¼ŒåŸºäºè¾“å…¥æ•°æ®ç”Ÿæˆè¿è¡Œç»“æœ"
        )
        parser_run.add_argument("--data-path", "-d", type=str, required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSV/Excelï¼‰")
        parser_run.add_argument("--domain", "-dm", type=str, default="general", help="ç›®æ ‡é¢†åŸŸ")
        parser_run.add_argument("--run-time", "-rt", type=int, default=300, help="æ¨¡æ‹Ÿè¿è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰")
        parser_run.add_argument("--output-path", "-o", type=str, default=f"{self.output_root}/run/run_result.csv", help="è¿è¡Œç»“æœä¿å­˜è·¯å¾„")
        
        # 3. analyzeå‘½ä»¤ï¼šä»…æ‰§è¡Œç»“æœåˆ†æ
        parser_analyze = subparsers.add_parser(
            "analyze", 
            help="ä»…æ‰§è¡Œç»“æœåˆ†æï¼ˆåŸºç¡€ç»Ÿè®¡+ç‰¹å¾é‡è¦æ€§+è‡ªé€‚åº”æ•ˆæœï¼‰",
            description="å¯¹æ™ºèƒ½ä½“è¿è¡Œç»“æœè¿›è¡Œæ·±åº¦ç»Ÿè®¡åˆ†æå’Œæ•ˆæœè¯„ä¼°"
        )
        parser_analyze.add_argument("--data-path", "-d", type=str, required=True, help="åˆ†ææ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSV/Excelï¼‰")
        parser_analyze.add_argument("--target-col", "-t", type=str, default="matter_output", help="ç›®æ ‡åˆ†æåˆ—å")
        parser_analyze.add_argument("--analysis-types", "-at", nargs="+", default=["basic", "feature"], choices=["basic", "feature", "adapt", "multimodal"], help="åˆ†æç±»å‹")
        parser_analyze.add_argument("--output-path", "-o", type=str, default=f"{self.output_root}/analysis", help="åˆ†æç»“æœä¿å­˜ç›®å½•")
        
        # 4. reportå‘½ä»¤ï¼šä»…ç”ŸæˆæŠ¥å‘Š
        parser_report = subparsers.add_parser(
            "report", 
            help="ä»…ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆæ”¯æŒMD/HTML/PDFæ ¼å¼ï¼‰",
            description="åŸºäºåˆ†æç»“æœç”Ÿæˆæ ‡å‡†åŒ–æŠ¥å‘Šï¼Œæ”¯æŒå¤šç§æ ¼å¼è¾“å‡º"
        )
        parser_report.add_argument("--analysis-path", "-a", type=str, required=True, help="åˆ†æç»“æœæ–‡ä»¶/ç›®å½•è·¯å¾„")
        parser_report.add_argument("--report-type", "-rt", type=str, default="comprehensive", choices=["run", "adapt", "multimodal", "comprehensive"], help="æŠ¥å‘Šç±»å‹")
        parser_report.add_argument("--format", "-f", nargs="+", default=["md", "html"], choices=["md", "html", "pdf"], help="æŠ¥å‘Šè¾“å‡ºæ ¼å¼")
        parser_report.add_argument("--with-plots", action="store_true", default=True, help="æ˜¯å¦åŒ…å«å¯è§†åŒ–å›¾è¡¨")
        parser_report.add_argument("--output-name", "-o", type=str, default=f"report_{time.strftime('%Y%m%d')}", help="æŠ¥å‘Šæ–‡ä»¶å‰ç¼€å")
        
        # 5. adaptå‘½ä»¤ï¼šä»…æ‰§è¡Œé¢†åŸŸè‡ªé€‚åº”
        parser_adapt = subparsers.add_parser(
            "adapt", 
            help="ä»…æ‰§è¡Œé¢†åŸŸè‡ªé€‚åº”ï¼ˆæ— ç›‘ç£å‚æ•°è°ƒæ•´ï¼‰",
            description="é’ˆå¯¹æŒ‡å®šé¢†åŸŸæ‰§è¡Œæ— ç›‘ç£è‡ªé€‚åº”ï¼Œä¼˜åŒ–æ™ºèƒ½ä½“å‚æ•°"
        )
        parser_adapt.add_argument("--data-path", "-d", type=str, required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„")
        parser_adapt.add_argument("--domain", "-dm", type=str, required=True, help="ç›®æ ‡é¢†åŸŸ")
        parser_adapt.add_argument("--adapt-iter", "-i", type=int, default=50, help="è‡ªé€‚åº”è¿­ä»£æ¬¡æ•°")
        parser_adapt.add_argument("--output-path", "-o", type=str, default=f"{self.output_root}/adapt/adapt_result.json", help="è‡ªé€‚åº”ç»“æœä¿å­˜è·¯å¾„")
        
        # 6. multimodalå‘½ä»¤ï¼šä»…æ‰§è¡Œå¤šæ¨¡æ€è§£æ
        parser_multimodal = subparsers.add_parser(
            "multimodal", 
            help="ä»…æ‰§è¡Œå¤šæ¨¡æ€æ•°æ®è§£æï¼ˆè¡¨æ ¼/æ–‡æœ¬/æ—¶åºæ•°æ®ï¼‰",
            description="è§£æå¤šæ¨¡æ€è¾“å…¥æ•°æ®ï¼Œç”Ÿæˆæ ‡å‡†åŒ–å¤šæ¨¡æ€æ•°æ®é›†"
        )
        parser_multimodal.add_argument("--data-paths", "-dp", nargs="+", required=True, help="å¤šæ¨¡æ€æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆCSV/Excel/TXTï¼‰")
        parser_multimodal.add_argument("--modal-types", "-mt", nargs="+", default=["table"], choices=["table", "text", "timeseries"], help="å„æ•°æ®æ¨¡æ€ç±»å‹")
        parser_multimodal.add_argument("--output-path", "-o", type=str, default=f"{self.output_root}/multimodal", help="å¤šæ¨¡æ€ç»“æœä¿å­˜ç›®å½•")
        
        # 7. historyå‘½ä»¤ï¼šæŸ¥çœ‹æ“ä½œå†å²
        parser_history = subparsers.add_parser(
            "history", 
            help="æŸ¥çœ‹å†å²æ“ä½œè®°å½•",
            description="æŸ¥çœ‹å½“å‰ä¼šè¯çš„æ“ä½œå†å²è®°å½•"
        )
        
        # 8. configå‘½ä»¤ï¼šæŸ¥çœ‹/ä¿®æ”¹é…ç½®
        parser_config = subparsers.add_parser(
            "config", 
            help="æŸ¥çœ‹/ä¿®æ”¹æ™ºèƒ½ä½“é…ç½®",
            description="æŸ¥çœ‹æˆ–ä¿®æ”¹UMCæ™ºèƒ½ä½“çš„åŸºç¡€é…ç½®å‚æ•°"
        )
        parser_config.add_argument("--show", "-s", action="store_true", default=True, help="æ˜¾ç¤ºå½“å‰é…ç½®")
        parser_config.add_argument("--set", "-se", nargs=2, metavar=("KEY", "VALUE"), help="è®¾ç½®é…ç½®é¡¹ï¼ˆå¦‚ --set output_dir ./new_outputï¼‰")
        
        return parser

    def run(self):
        """æ‰§è¡Œå‘½ä»¤è¡Œæ“ä½œï¼ˆæ ¸å¿ƒå…¥å£ï¼‰"""
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = self.parser.parse_args()
        
        # è®°å½•æ“ä½œå¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # æ ¹æ®å‘½ä»¤åˆ†å‘æ‰§è¡Œ
            if args.command == "all":
                self._execute_all(args)
            elif args.command == "run":
                self._execute_run(args)
            elif args.command == "analyze":
                self._execute_analyze(args)
            elif args.command == "report":
                self._execute_report(args)
            elif args.command == "adapt":
                self._execute_adapt(args)
            elif args.command == "multimodal":
                self._execute_multimodal(args)
            elif args.command == "history":
                self._show_history()
            elif args.command == "config":
                self._manage_config(args)
            
            # è®°å½•æ“ä½œå†å²
            self.operation_history.append({
                "command": args.command,
                "arguments": vars(args),
                "start_time": start_time,
                "end_time": time.time(),
                "duration": round(time.time() - start_time, 2),
                "status": "success"
            })
            
            # è¾“å‡ºå®Œæˆä¿¡æ¯
            print(f"\nğŸ‰ æ“ä½œå®Œæˆï¼æ€»è€—æ—¶ï¼š{round(time.time() - start_time, 2)}ç§’")
            print(f"ğŸ“ è¾“å‡ºç›®å½•ï¼š{self.output_root}")
            
        except Exception as e:
            # è®°å½•å¤±è´¥æ“ä½œ
            self.operation_history.append({
                "command": args.command if hasattr(args, "command") else "unknown",
                "arguments": vars(args) if hasattr(args, "__dict__") else {},
                "start_time": start_time,
                "end_time": time.time(),
                "duration": round(time.time() - start_time, 2),
                "status": "failed",
                "error": str(e)
            })
            
            # è¾“å‡ºé”™è¯¯ä¿¡æ¯
            print(f"\nâŒ æ“ä½œå¤±è´¥ï¼š{e}")
            print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ -h å‚æ•°æŸ¥çœ‹å¸®åŠ©ï¼Œä¾‹å¦‚ï¼špython universal_cmd.py analyze -h")
            sys.exit(1)

    # ------------------------------ å‘½ä»¤æ‰§è¡Œé€»è¾‘ ------------------------------
    def _execute_all(self, args: argparse.Namespace):
        """æ‰§è¡Œallå‘½ä»¤ï¼šä¸€ç«™å¼å®Œæˆè¿è¡Œâ†’åˆ†æâ†’æŠ¥å‘Š"""
        print("\nğŸš€ å¼€å§‹UMCæ™ºèƒ½ä½“å…¨æµç¨‹æ“ä½œ...")
        
        # æ­¥éª¤1ï¼šè¿è¡Œæ™ºèƒ½ä½“
        print("\n===== æ­¥éª¤1/3ï¼šè¿è¡Œæ™ºèƒ½ä½“ =====")
        run_args = argparse.Namespace(
            data_path=args.data_path,
            domain=args.domain,
            run_time=300,
            output_path=f"{self.output_root}/run/{args.output_name}_run.csv"
        )
        run_result = self._execute_run(run_args, return_result=True)
        
        # æ­¥éª¤2ï¼šç»“æœåˆ†æ
        print("\n===== æ­¥éª¤2/3ï¼šç»“æœåˆ†æ =====")
        analyze_args = argparse.Namespace(
            data_path=run_args.output_path,
            target_col=args.target_col,
            analysis_types=["basic", "feature", "adapt"],
            output_path=f"{self.output_root}/analysis/{args.output_name}"
        )
        analyze_result = self._execute_analyze(analyze_args, return_result=True)
        
        # æ­¥éª¤3ï¼šç”ŸæˆæŠ¥å‘Š
        print("\n===== æ­¥éª¤3/3ï¼šç”ŸæˆæŠ¥å‘Š =====")
        report_args = argparse.Namespace(
            analysis_path=f"{self.output_root}/analysis/{args.output_name}",
            report_type="comprehensive",
            format=args.report_formats,
            with_plots=args.with_plots,
            output_name=args.output_name
        )
        self._execute_report(report_args)
        
        # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
        print("\nğŸ“Š å…¨æµç¨‹æ“ä½œæ±‡æ€»ï¼š")
        print(f"  â€¢ æ™ºèƒ½ä½“è¿è¡Œç»“æœï¼š{run_args.output_path}")
        print(f"  â€¢ åˆ†æç»“æœç›®å½•ï¼š{analyze_args.output_path}")
        print(f"  â€¢ æŠ¥å‘Šæ–‡ä»¶ï¼š{self.output_root}/reports/{args.output_name}.*")
        print(f"  â€¢ å¯è§†åŒ–å›¾è¡¨ï¼š{self.output_root}/reports/report_plots/")

    def _execute_run(self, args: argparse.Namespace, return_result: bool = False) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œrunå‘½ä»¤ï¼šè¿è¡Œæ™ºèƒ½ä½“"""
        print(f"\nâ–¶ï¸  è¿è¡ŒUMCæ™ºèƒ½ä½“ï¼ˆé¢†åŸŸï¼š{args.domain}ï¼‰...")
        
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not os.path.exists(args.data_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.data_path}")
        
        # è¯»å–è¾“å…¥æ•°æ®
        try:
            if args.data_path.endswith(".csv"):
                input_data = pd.read_csv(args.data_path, encoding="utf-8")
            elif args.data_path.endswith((".xlsx", ".xls")):
                input_data = pd.read_excel(args.data_path)
            else:
                raise ValueError("ä»…æ”¯æŒCSV/Excelæ ¼å¼æ•°æ®æ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–æ•°æ®å¤±è´¥ï¼š{e}")
        
        # æ¨¡æ‹Ÿæ™ºèƒ½ä½“è¿è¡Œï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
        print(f"ğŸ“¥ åŠ è½½æ•°æ®ï¼š{len(input_data)}è¡Œ Ã— {len(input_data.columns)}åˆ—")
        print(f"â±ï¸  æ¨¡æ‹Ÿè¿è¡Œæ—¶é•¿ï¼š{args.run_time}ç§’")
        
        # ç”Ÿæˆè¿è¡Œç»“æœï¼ˆæ·»åŠ æ—¶é—´æˆ³å’Œè¿è¡ŒæŒ‡æ ‡ï¼‰
        run_data = input_data.copy()
        run_data["timestamp"] = pd.date_range(start=pd.Timestamp.now(), periods=len(run_data), freq="1s")
        run_data["run_status"] = "normal"
        run_data["metabolic_efficiency"] = np.random.rand(len(run_data)) * 0.9 + 0.1  # ä»£è°¢æ•ˆç‡ 0.1-1.0
        run_data["domain_adapt_score"] = np.random.rand(len(run_data)) * 0.8 + 0.2   # é¢†åŸŸé€‚é…å¾—åˆ† 0.2-1.0
        
        # ä¿å­˜è¿è¡Œç»“æœ
        os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
        run_data.to_csv(args.output_path, index=False, encoding="utf-8")
        print(f"âœ… è¿è¡Œç»“æœå·²ä¿å­˜ï¼š{args.output_path}")
        
        # æ„å»ºè¿è¡Œç»“æœå­—å…¸
        run_result = {
            "start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "domain": args.domain,
            "data_shape": (len(run_data), len(run_data.columns)),
            "core_metrics": {
                "avg_metabolic_efficiency": round(run_data["metabolic_efficiency"].mean(), 3),
                "avg_adapt_score": round(run_data["domain_adapt_score"].mean(), 3),
                "data_coverage": round(len(run_data.dropna()) / len(run_data), 3),
                "run_success_rate": 1.0
            },
            "output_path": args.output_path
        }
        
        # ä¿å­˜è¿è¡Œå…ƒæ•°æ®
        meta_path = args.output_path.replace(".csv", "_meta.json")
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(run_result, f, ensure_ascii=False, indent=2)
        
        if return_result:
            return run_result
        return None

    def _execute_analyze(self, args: argparse.Namespace, return_result: bool = False) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œanalyzeå‘½ä»¤ï¼šç»“æœåˆ†æ"""
        if not self.analyzer:
            raise RuntimeError("ResultAnalyzeræ¨¡å—æœªåŠ è½½ï¼Œæ— æ³•æ‰§è¡Œåˆ†æ")
        
        print(f"\nâ–¶ï¸  æ‰§è¡Œç»“æœåˆ†æï¼ˆç›®æ ‡åˆ—ï¼š{args.target_col}ï¼‰...")
        
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not os.path.exists(args.data_path):
            raise FileNotFoundError(f"åˆ†ææ•°æ®ä¸å­˜åœ¨ï¼š{args.data_path}")
        
        # è¯»å–åˆ†ææ•°æ®
        try:
            if args.data_path.endswith(".csv"):
                analyze_data = pd.read_csv(args.data_path, encoding="utf-8")
            elif args.data_path.endswith((".xlsx", ".xls")):
                analyze_data = pd.read_excel(args.data_path)
            else:
                raise ValueError("ä»…æ”¯æŒCSV/Excelæ ¼å¼æ•°æ®æ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–åˆ†ææ•°æ®å¤±è´¥ï¼š{e}")
        
        # éªŒè¯ç›®æ ‡åˆ—
        if args.target_col not in analyze_data.columns:
            raise ValueError(f"ç›®æ ‡åˆ—ä¸å­˜åœ¨ï¼š{args.target_col}ï¼Œå¯ç”¨åˆ—ï¼š{analyze_data.columns.tolist()}")
        
        # æ‰§è¡ŒæŒ‡å®šç±»å‹çš„åˆ†æ
        analysis_results = {}
        os.makedirs(args.output_path, exist_ok=True)
        
        # 1. åŸºç¡€ç»Ÿè®¡åˆ†æ
        if "basic" in args.analysis_types:
            print("ğŸ“Š æ‰§è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ...")
            basic_result = self.analyzer.basic_statistical_analysis(
                analyze_data,
                target_cols=[col for col in analyze_data.columns if col in ["metabolic_efficiency", "domain_adapt_score", args.target_col]],
                save_name="basic_analysis"
            )
            analysis_results["basic"] = basic_result
            basic_path = os.path.join(args.output_path, "basic_analysis.json")
            with open(basic_path, "w", encoding="utf-8") as f:
                json.dump(basic_result, f, ensure_ascii=False, indent=2)
        
        # 2. ç‰¹å¾é‡è¦æ€§åˆ†æ
        if "feature" in args.analysis_types:
            print("ğŸ” æ‰§è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ...")
            feature_result = self.analyzer.feature_importance_analysis(
                analyze_data,
                target_col=args.target_col,
                save_name="feature_importance"
            )
            analysis_results["feature"] = feature_result
            feature_path = os.path.join(args.output_path, "feature_importance.json")
            with open(feature_path, "w", encoding="utf-8") as f:
                json.dump(feature_result, f, ensure_ascii=False, indent=2)
        
        # 3. é¢†åŸŸè‡ªé€‚åº”åˆ†æï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
        if "adapt" in args.analysis_types:
            print("ğŸŒ æ‰§è¡Œé¢†åŸŸè‡ªé€‚åº”åˆ†æ...")
            adapt_result_data = {
                "start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "data_info": {"sample_count": len(analyze_data), "feature_cols": analyze_data.columns.tolist()},
                "domain_match": {"domain": "general", "similarity": round(np.random.rand() * 0.3 + 0.7, 3)},
                "adapt_params": {
                    "metabolism_params": {"core_factor_weight": 0.85, "stability_threshold": 0.80},
                    "strategy_params": {"domain_weight": 0.9, "efficiency_weight": 0.7}
                },
                "adapt_effect": {
                    "metabolic_stability": round(np.random.rand() * 0.2 + 0.7, 3),
                    "result_consistency": round(np.random.rand() * 0.2 + 0.75, 3),
                    "run_efficiency": round(np.random.rand() * 0.2 + 0.8, 3),
                    "performance_rate": round(np.random.rand() * 0.2 + 0.78, 3),
                    "comprehensive_score": round(np.random.rand() * 0.15 + 0.75, 3)
                },
                "is_adapt_successful": True
            }
            adapt_result = self.analyzer.domain_adaptation_analysis(
                adapt_result_data,
                save_name="domain_adaptation"
            )
            analysis_results["adapt"] = adapt_result
            adapt_path = os.path.join(args.output_path, "domain_adaptation.json")
            with open(adapt_path, "w", encoding="utf-8") as f:
                json.dump(adapt_result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š{args.output_path}")
        
        if return_result:
            return analysis_results
        return None

    def _execute_report(self, args: argparse.Namespace):
        """æ‰§è¡Œreportå‘½ä»¤ï¼šç”ŸæˆæŠ¥å‘Š"""
        if not self.report_generator:
            raise RuntimeError("ReportGeneratoræ¨¡å—æœªåŠ è½½ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        
        print(f"\nâ–¶ï¸  ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆç±»å‹ï¼š{args.report_type}ï¼Œæ ¼å¼ï¼š{args.format}ï¼‰...")
        
        # åŠ è½½åˆ†æç»“æœ
        analysis_results = {}
        if os.path.isdir(args.analysis_path):
            # ç›®å½•ï¼šåŠ è½½æ‰€æœ‰JSONæ–‡ä»¶
            for file in os.listdir(args.analysis_path):
                if file.endswith(".json"):
                    file_path = os.path.join(args.analysis_path, file)
                    with open(file_path, "r", encoding="utf-8") as f:
                        analysis_type = file.replace(".json", "").split("_")[0]
                        analysis_results[analysis_type] = json.load(f)
        elif os.path.isfile(args.analysis_path) and args.analysis_path.endswith(".json"):
            # æ–‡ä»¶ï¼šåŠ è½½å•ä¸ªJSON
            with open(args.analysis_path, "r", encoding="utf-8") as f:
                analysis_results["single"] = json.load(f)
        else:
            raise ValueError(f"åˆ†æç»“æœè·¯å¾„æ— æ•ˆï¼š{args.analysis_path}ï¼ˆéœ€ä¸ºJSONæ–‡ä»¶æˆ–åŒ…å«JSONçš„ç›®å½•ï¼‰")
        
        # ç”Ÿæˆå¯¹åº”ç±»å‹çš„æŠ¥å‘Š
        report_config = {
            "project_name": f"UMCæ™ºèƒ½ä½“{args.report_type}åˆ†ææŠ¥å‘Š",
            "generate_time": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        if args.report_type == "comprehensive":
            # ç»¼åˆæŠ¥å‘Š
            report_paths = self.report_generator.generate_comprehensive_report(
                report_config,
                report_name=args.output_name,
                format_list=args.format
            )
        elif args.report_type == "run":
            # è¿è¡ŒæŠ¥å‘Š
            run_results = analysis_results.get("basic", {})
            report_paths = self.report_generator.generate_run_report(
                run_results,
                report_name=args.output_name,
                format_list=args.format,
                with_plots=args.with_plots
            )
        elif args.report_type == "adapt":
            # è‡ªé€‚åº”æŠ¥å‘Š
            adapt_results = analysis_results.get("adapt", {})
            report_paths = self.report_generator.generate_adapt_report(
                adapt_results,
                report_name=args.output_name,
                format_list=args.format,
                with_plots=args.with_plots
            )
        elif args.report_type == "multimodal":
            # å¤šæ¨¡æ€æŠ¥å‘Šï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
            multimodal_data = {
                "table": pd.DataFrame(np.random.rand(50, 3), columns=["f1", "f2", "f3"]),
                "text": pd.DataFrame(np.random.rand(20, 2), columns=["t1", "t2"])
            }
            report_paths = self.report_generator.generate_multimodal_report(
                multimodal_data,
                report_name=args.output_name,
                format_list=args.format,
                with_plots=args.with_plots
            )
        
        # è¾“å‡ºæŠ¥å‘Šè·¯å¾„
        print("ğŸ“„ ç”Ÿæˆçš„æŠ¥å‘Šï¼š")
        for fmt, path in report_paths.items():
            print(f"  â€¢ {fmt.upper()}æ ¼å¼ï¼š{path}")

    def _execute_adapt(self, args: argparse.Namespace):
        """æ‰§è¡Œadaptå‘½ä»¤ï¼šé¢†åŸŸè‡ªé€‚åº”"""
        print(f"\nâ–¶ï¸  æ‰§è¡Œ{args.domain}é¢†åŸŸè‡ªé€‚åº”ï¼ˆè¿­ä»£æ¬¡æ•°ï¼š{args.adapt_iter}ï¼‰...")
        
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not os.path.exists(args.data_path):
            raise FileNotFoundError(f"è¾“å…¥æ•°æ®ä¸å­˜åœ¨ï¼š{args.data_path}")
        
        # è¯»å–æ•°æ®
        try:
            input_data = pd.read_csv(args.data_path, encoding="utf-8") if args.data_path.endswith(".csv") else pd.read_excel(args.data_path)
        except Exception as e:
            raise ValueError(f"è¯»å–æ•°æ®å¤±è´¥ï¼š{e}")
        
        # æ¨¡æ‹Ÿé¢†åŸŸè‡ªé€‚åº”è¿‡ç¨‹
        adapt_progress = []
        for i in range(args.adapt_iter):
            # æ¨¡æ‹Ÿæ¯æ¬¡è¿­ä»£çš„é€‚é…å¾—åˆ†
            adapt_score = min(0.99, 0.5 + (i / args.adapt_iter) * 0.5 + np.random.rand() * 0.1)
            adapt_progress.append({
                "iteration": i+1,
                "adapt_score": round(adapt_score, 3),
                "params_adjusted": ["core_factor_weight", "stability_threshold"] if i % 10 == 0 else []
            })
            
            # è¾“å‡ºè¿›åº¦
            if (i+1) % 10 == 0 or i+1 == args.adapt_iter:
                print(f"  è¿›åº¦ï¼š{i+1}/{args.adapt_iter} | å½“å‰é€‚é…å¾—åˆ†ï¼š{adapt_score:.3f}")
        
        # æ„å»ºè‡ªé€‚åº”ç»“æœ
        adapt_result = {
            "start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "domain": args.domain,
            "adapt_iterations": args.adapt_iter,
            "data_shape": input_data.shape,
            "final_adapt_score": adapt_progress[-1]["adapt_score"],
            "adapt_success": adapt_progress[-1]["adapt_score"] >= 0.7,
            "adapt_progress": adapt_progress,
            "optimized_params": {
                "metabolism_params": {
                    "core_factor_weight": round(np.random.rand() * 0.4 + 0.6, 3),
                    "stability_threshold": round(np.random.rand() * 0.3 + 0.7, 3),
                    "cycle_speed": round(np.random.rand() * 0.1 + 0.05, 3)
                },
                "domain_strategy": {
                    f"{args.domain}_weight": 0.9,
                    "general_weight": 0.1
                }
            }
        }
        
        # ä¿å­˜è‡ªé€‚åº”ç»“æœ
        os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
        with open(args.output_path, "w", encoding="utf-8") as f:
            json.dump(adapt_result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é¢†åŸŸè‡ªé€‚åº”å®Œæˆï¼")
        print(f"  â€¢ ç›®æ ‡é¢†åŸŸï¼š{args.domain}")
        print(f"  â€¢ æœ€ç»ˆé€‚é…å¾—åˆ†ï¼š{adapt_result['final_adapt_score']:.3f}")
        print(f"  â€¢ è‡ªé€‚åº”æˆåŠŸï¼š{adapt_result['adapt_success']}")
        print(f"  â€¢ ç»“æœå·²ä¿å­˜ï¼š{args.output_path}")

    def _execute_multimodal(self, args: argparse.Namespace):
        """æ‰§è¡Œmultimodalå‘½ä»¤ï¼šå¤šæ¨¡æ€è§£æ"""
        print(f"\nâ–¶ï¸  æ‰§è¡Œå¤šæ¨¡æ€æ•°æ®è§£æï¼ˆæ¨¡æ€ç±»å‹ï¼š{args.modal_types}ï¼‰...")
        
        # éªŒè¯è¾“å…¥è·¯å¾„æ•°é‡
        if len(args.data_paths) != len(args.modal_types):
            raise ValueError(f"æ•°æ®è·¯å¾„æ•°é‡ï¼ˆ{len(args.data_paths)}ï¼‰éœ€ä¸æ¨¡æ€ç±»å‹æ•°é‡ï¼ˆ{len(args.modal_types)}ï¼‰ä¸€è‡´")
        
        # è§£æå„æ¨¡æ€æ•°æ®
        multimodal_data = {}
        for i, (data_path, modal_type) in enumerate(zip(args.data_paths, args.modal_types)):
            # éªŒè¯æ–‡ä»¶
            if not os.path.exists(data_path):
                raise FileNotFoundError(f"æ¨¡æ€æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")
            
            # è¯»å–æ•°æ®
            try:
                if data_path.endswith(".csv"):
                    data = pd.read_csv(data_path, encoding="utf-8")
                elif data_path.endswith((".xlsx", ".xls")):
                    data = pd.read_excel(data_path)
                elif data_path.endswith(".txt"):
                    # æ–‡æœ¬æ•°æ®ç‰¹æ®Šå¤„ç†
                    with open(data_path, "r", encoding="utf-8") as f:
                        lines = f.readlines()
                    data = pd.DataFrame({"text": [line.strip() for line in lines]})
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{data_path}")
                
                multimodal_data[f"{modal_type}_{i+1}"] = data
                print(f"  âœ… è§£æ{modal_type}æ¨¡æ€æ•°æ®ï¼š{data_path}ï¼ˆ{len(data)}è¡Œï¼‰")
                
            except Exception as e:
                raise ValueError(f"è§£æ{modal_type}æ¨¡æ€æ•°æ®å¤±è´¥ï¼š{e}")
        
        # ä¿å­˜å¤šæ¨¡æ€ç»“æœ
        os.makedirs(args.output_path, exist_ok=True)
        
        # ä¿å­˜å„æ¨¡æ€æ•°æ®
        for modal_name, data in multimodal_data.items():
            save_path = os.path.join(args.output_path, f"{modal_name}.csv")
            data.to_csv(save_path, index=False, encoding="utf-8")
        
        # ç”Ÿæˆå¤šæ¨¡æ€å…ƒæ•°æ®
        multimodal_meta = {
            "generate_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "modal_count": len(multimodal_data),
            "modal_info": {
                name: {
                    "sample_count": len(data),
                    "feature_count": len(data.columns),
                    "data_type": modal_name.split("_")[0]
                } for name, data in multimodal_data.items()
            },
            "fusion_quality": {
                "consistency_score": round(np.random.rand() * 0.2 + 0.7, 3),
                "complementarity_score": round(np.random.rand() * 0.2 + 0.8, 3),
                "data_quality_score": round(np.random.rand() * 0.1 + 0.85, 3)
            }
        }
        
        # ä¿å­˜å…ƒæ•°æ®
        meta_path = os.path.join(args.output_path, "multimodal_meta.json")
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(multimodal_meta, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å¤šæ¨¡æ€è§£æå®Œæˆï¼")
        print(f"  â€¢ è§£ææ¨¡æ€æ•°ï¼š{multimodal_meta['modal_count']}")
        print(f"  â€¢ èåˆè´¨é‡å¾—åˆ†ï¼š{multimodal_meta['fusion_quality']['data_quality_score']:.3f}")
        print(f"  â€¢ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output_path}")

    def _show_history(self):
        """æ˜¾ç¤ºæ“ä½œå†å²"""
        print("\nğŸ“œ UMCæ™ºèƒ½ä½“æ“ä½œå†å²ï¼š")
        if not self.operation_history:
            print("  æš‚æ— æ“ä½œè®°å½•")
            return
        
        for idx, history in enumerate(self.operation_history):
            status_icon = "âœ…" if history["status"] == "success" else "âŒ"
            print(f"\n  {idx+1}. {status_icon} å‘½ä»¤ï¼š{history['command']}")
            print(f"     è€—æ—¶ï¼š{history['duration']}ç§’")
            print(f"     çŠ¶æ€ï¼š{history['status'].upper()}")
            if history["status"] == "failed":
                print(f"     é”™è¯¯ï¼š{history['error']}")
            print(f"     å‚æ•°ï¼š{json.dumps(history['arguments'], ensure_ascii=False, indent=4)}")

    def _manage_config(self, args: argparse.Namespace):
        """ç®¡ç†é…ç½®"""
        # å½“å‰é…ç½®
        current_config = {
            "output_root": self.output_root,
            "supported_commands": ["all", "run", "analyze", "report", "adapt", "multimodal", "history", "config"],
            "supported_report_formats": ["md", "html", "pdf"],
            "default_domain": "general",
            "default_target_column": "matter_output"
        }
        
        if args.show:
            print("\nâš™ï¸ UMCæ™ºèƒ½ä½“å½“å‰é…ç½®ï¼š")
            print(json.dumps(current_config, ensure_ascii=False, indent=2))
        
        if args.set:
            key, value = args.set
            if key in current_config:
                # éªŒè¯å€¼ç±»å‹
                if key == "output_root":
                    os.makedirs(value, exist_ok=True)
                    self.output_root = value
                    current_config[key] = value
                    print(f"\nâœ… é…ç½®å·²æ›´æ–°ï¼š{key} = {value}")
                else:
                    print(f"\nâš ï¸  é…ç½®é¡¹{key}ä¸æ”¯æŒä¿®æ”¹")
            else:
                print(f"\nâŒ æ— æ•ˆçš„é…ç½®é¡¹ï¼š{key}ï¼Œå¯ç”¨é…ç½®é¡¹ï¼š{list(current_config.keys())}")

# å‘½ä»¤è¡Œå…¥å£
if __name__ == "__main__":
    # åˆå§‹åŒ–é€šç”¨å‘½ä»¤è¡Œæ§åˆ¶å™¨
    cmd = UniversalCmd()
    
    # æ‰§è¡Œå‘½ä»¤
    cmd.run()